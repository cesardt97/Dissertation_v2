{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from utils_pcmci import *\n",
    "\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters to run ####\n",
    "# seasons_mask = {'DJF': [12, 1, 2], 'MAM': [3, 4, 5], 'JJA': [6, 7, 8], 'SON': [9, 10, 11]}     \n",
    "seasons_mask = {'FMA': [2, 3, 4], 'MJJ': [5, 6, 7], 'ASO': [8, 9, 10], 'NDJ': [11, 12, 1]} \n",
    "              \n",
    "variables = [\n",
    "    'sst', \n",
    "    # 'mtpr', \n",
    "    'prate',\n",
    "    # 'msl'\n",
    "    ]\n",
    "\n",
    "model_name = {\n",
    "    'sst': 'FULL_ERA5_SST_1940-2024_converted_detrend.nc',\n",
    "    # 'mtpr': 'ERA5_mean_precipitation_1940-2024_converted_detrend.nc',\n",
    "    'prate': 'PRATE_NCEP_NCAR_Reanalysis_1948-2024.nc',\n",
    "    # 'msl': 'ERA5_mean_SLP_1940-2024_converted_detrend.nc'\n",
    "    }\n",
    "\n",
    "target = 'prate_FMA'\n",
    "ip = 0\n",
    "n_comps = {'sst': 15, 'prate': 5}\n",
    "n_splits = 5\n",
    "mask = 'unmasked'\n",
    "method_arg = 'pcmci'\n",
    "sequence_length = 12\n",
    "period_length = 38 * 2\n",
    "model = \"_\".join(variables)\n",
    "months = \"_\".join(seasons_mask.keys())\n",
    "\n",
    "\n",
    "# Define the number of components and the names of the components\n",
    "total_comps = int((sum(n_comps.values()) / len(n_comps)) * len(seasons_mask) * len(model_name))\n",
    "\n",
    "selected_components=['c'+str(i) for i in range(1,total_comps+1)] #  + 2 to include the component that is the precipitation rate\n",
    "comp_names = [f'{var}_{months}' for months in seasons_mask.keys() for var in variables for _ in range(n_comps[var])]\n",
    "comps_order_file = pd.DataFrame({'comp_number': ['c'+str(i) for i in range(1, total_comps+1)], # + 2 to include the component that is the precipitation rate\n",
    "                                 'comps': [i for i in range(total_comps)], # + 1 to include the component that is the precipitation rate\n",
    "                                 'name': [ i + '_' + j for i, j in zip(selected_components, comp_names)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fulldata shape = (912, 80)\n",
      "Fulldata masked shape = (912, 80)\n"
     ]
    }
   ],
   "source": [
    "# Load the \n",
    "datadict, fulldata, fulldata_mask = load_varimax_data(variables, seasons_mask, model_name, n_comps, mask)\n",
    "print(\"Fulldata shape = %s\" % str(fulldata.shape))\n",
    "print(\"Fulldata masked shape = %s\" % str(fulldata_mask.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
      "/var/folders/5s/g78x_2cn3b56vfckpn9d5vpr0000gn/T/ipykernel_47929/448644342.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe with lagged variables\n",
    "fulldata_pd = pd.DataFrame(fulldata, columns=comps_order_file['comp_number'])\n",
    "\n",
    "def create_lagged_features(data, lags):\n",
    "    temp = pd.DataFrame(index=data.index)\n",
    "    for col in data.columns:\n",
    "        for lag in range(1, lags+1):\n",
    "            temp[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "lagged_df = create_lagged_features(fulldata_pd, 5)\n",
    "combined_df = pd.concat([fulldata_pd, lagged_df], axis=1).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.176e+03, tolerance: 1.827e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.13279\n",
      "Selected features: [  0   1   2   3   7   9  11  12  13  19  20  21  23  24  25  26  28  29\n",
      "  30  32  34  38  41  43  44  46  52  53  57  59  62  63  64  67  68  69\n",
      "  70  72  73  79  84  93  98  99 101 104 120 122 124 125 126 129 130 142\n",
      " 148 149 151 165 177 188 198 201 203 204 206 207 208 228 231 234 236 239\n",
      " 240 241 243 246 252 260 261 267 268 274 293 294 303 304 322 323 326 329\n",
      " 330 339 341 346 347 353 354 360 366 368 370 371 372 373 398 408 412 422\n",
      " 423 425 427 434 450 452 456 457 460 461 468 472]\n",
      "Test set R^2 with selected features: 0.99687\n",
      "\n",
      "Component 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.794e+04, tolerance: 1.534e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.12279\n",
      "Selected features: [  0   2   3   8   9  10  12  13  14  21  22  23  26  28  30  32  33  38\n",
      "  40  41  44  45  46  48  49  50  51  52  58  59  60  61  62  63  64  65\n",
      "  66  67  68  69  72  73  77  84  88  93  96 102 103 114 119 126 135 136\n",
      " 139 140 142 144 145 149 152 154 157 162 165 166 167 171 174 178 183 189\n",
      " 190 196 197 206 208 209 223 229 231 232 233 234 235 237 238 244 253 255\n",
      " 279 289 291 294 299 301 303 304 313 314 318 319 320 324 325 328 329 331\n",
      " 332 333 339 345 346 349 362 365 366 370 373 374 375 376 386 400 402 408\n",
      " 413 415 420 428 436 446 450 459 464 472]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.835e+03, tolerance: 1.244e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.98977\n",
      "\n",
      "Component 2\n",
      "Selected alpha: 0.17346\n",
      "Selected features: [  0   2   4   5   6   7   8  10  11  20  23  25  27  28  29  30  31  32\n",
      "  33  39  43  47  48  50  52  57  58  60  62  63  65  67  69  70  72  73\n",
      "  88  89  99 102 115 116 119 123 126 145 146 148 159 164 166 172 183 184\n",
      " 185 187 192 193 195 216 217 227 228 233 239 240 243 246 247 248 250 252\n",
      " 262 271 272 278 284 288 294 296 304 326 330 331 334 338 346 349 363 366\n",
      " 369 375 384 399 402 426 428 432 433 453 464 472 478]\n",
      "Test set R^2 with selected features: 0.99117\n",
      "\n",
      "Component 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.825e+02, tolerance: 4.119e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.638e+02, tolerance: 4.119e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1502.1596570533293, tolerance: 645.556149232568\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1213.479759118476, tolerance: 645.556149232568\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1389.8887324401003, tolerance: 645.556149232568\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 721.4601427380258, tolerance: 645.556149232568\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1574.1914164011105, tolerance: 769.1103648258253\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1450.2499932921055, tolerance: 769.1103648258253\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.686e+04, tolerance: 9.731e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.09373\n",
      "Selected features: [  0   1   3   4   5   6   8   9  10  11  12  13  15  19  20  23  25  26\n",
      "  28  30  31  32  33  35  36  39  40  42  45  46  47  48  51  53  56  57\n",
      "  58  59  60  61  62  63  64  65  66  68  69  73  75  76  88  94  96 101\n",
      " 104 119 121 122 125 127 129 137 139 148 170 175 190 199 203 205 206 209\n",
      " 210 214 217 218 223 231 234 237 239 243 248 249 262 263 266 268 276 277\n",
      " 279 280 281 283 284 285 292 294 299 300 303 305 315 319 324 325 328 331\n",
      " 332 333 334 337 338 344 348 351 353 363 365 368 369 371 373 378 384 385\n",
      " 388 398 400 402 409 415 419 421 425 426 427 428 431 438 439 440 445 446\n",
      " 448 451 456 461 464 467 468 474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 584.4318481585324, tolerance: 271.4684767909079\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 547.5244791824352, tolerance: 271.4684767909079\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 678.9945021428866, tolerance: 657.6733924048006\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.298e+03, tolerance: 7.691e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.96782\n",
      "\n",
      "Component 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.286e+03, tolerance: 9.477e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.09341\n",
      "Selected features: [  0   3   4   6   7   8   9  10  11  16  17  19  20  21  23  24  25  27\n",
      "  28  31  35  36  37  40  44  45  46  47  49  50  52  53  60  62  63  64\n",
      "  66  67  69  72  73  77  85  95  99 100 103 114 115 116 117 132 133 137\n",
      " 149 169 174 175 196 199 200 208 217 218 223 231 232 234 237 238 239 243\n",
      " 259 262 268 272 273 275 280 285 287 289 290 291 297 298 299 300 310 311\n",
      " 321 322 324 331 334 335 346 347 349 361 367 368 371 374 375 378 379 380\n",
      " 386 394 398 401 402 406 411 415 417 424 426 428 429 430 431 433 444 447\n",
      " 451 452 463 468 471 472 474 475]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.301e+02, tolerance: 7.305e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99082\n",
      "\n",
      "Component 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.051e+03, tolerance: 7.656e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.08315\n",
      "Selected features: [  0   2   3   4   5   8   9  10  11  13  20  22  23  25  27  29  30  31\n",
      "  32  35  36  37  40  41  42  43  44  45  46  48  49  50  51  53  56  57\n",
      "  60  61  62  63  64  68  69  70  71  73  76  77  90  93  94  95  98 101\n",
      " 104 107 109 110 112 114 125 129 130 132 133 138 143 144 145 147 148 153\n",
      " 169 174 177 178 193 194 197 200 201 203 206 207 212 223 226 232 236 237\n",
      " 238 239 245 249 252 253 260 261 271 272 273 275 277 278 289 293 296 299\n",
      " 305 314 315 318 319 322 327 329 332 338 344 348 349 362 369 370 374 376\n",
      " 377 378 389 396 405 406 414 419 429 432 438 441 442 444 449 451 452 454\n",
      " 456 457 458 460 463 464 465 467 472]\n",
      "Test set R^2 with selected features: 0.96702\n",
      "\n",
      "Component 6\n",
      "Selected alpha: 0.14752\n",
      "Selected features: [  2   3   8   9  10  11  12  13  23  25  26  29  30  31  32  33  39  41\n",
      "  42  43  44  45  46  48  53  57  58  60  63  65  67  72  73  75  76  85\n",
      "  97 107 109 113 119 126 127 128 143 150 153 174 202 203 204 214 215 216\n",
      " 224 225 227 232 233 236 237 247 258 262 263 267 268 269 270 288 293 314\n",
      " 316 317 329 337 338 343 344 348 352 355 356 360 370 372 374 375 378 392\n",
      " 405 411 419 433 440 441 442 449 460 469 470 471 474 476]\n",
      "Test set R^2 with selected features: 0.99042\n",
      "\n",
      "Component 7\n",
      "Selected alpha: 0.12917\n",
      "Selected features: [  2   3   4   5   8   9  10  15  16  17  18  19  22  24  27  28  29  31\n",
      "  32  33  39  40  42  44  48  49  50  51  52  60  62  67  68  70  71  73\n",
      "  89  96  97  98 101 104 107 114 115 116 118 119 133 140 143 148 159 161\n",
      " 168 175 177 178 191 192 199 219 223 224 225 226 229 233 239 253 266 268\n",
      " 271 279 299 300 302 303 304 305 307 312 313 319 321 322 328 331 333 337\n",
      " 338 339 346 348 354 361 369 374 376 377 384 388 391 393 400 401 403 421\n",
      " 424 425 428 434 442 443 446 447 449 460 461 468 470 471]\n",
      "Test set R^2 with selected features: 0.98679\n",
      "\n",
      "Component 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.799e+03, tolerance: 7.677e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.08555\n",
      "Selected features: [  0   1   2   4   7   8  10  12  16  18  23  27  28  30  31  32  33  35\n",
      "  37  38  39  41  43  44  46  47  48  49  50  52  59  60  63  64  66  67\n",
      "  68  69  70  72  73  76  80 101 103 110 111 114 117 119 120 123 124 129\n",
      " 131 143 144 147 148 167 171 177 178 192 195 199 206 214 217 218 219 222\n",
      " 223 226 227 231 232 234 235 237 239 244 249 253 258 259 265 269 271 275\n",
      " 279 281 282 288 289 295 299 300 303 309 314 318 323 324 325 329 336 338\n",
      " 339 340 343 344 346 348 349 350 358 364 369 373 374 377 378 387 388 391\n",
      " 396 409 411 413 414 416 421 423 424 425 427 432 435 441 444 447 453 455\n",
      " 456 457 460 461 462 466 469 471 472 475 477]\n",
      "Test set R^2 with selected features: 0.96474\n",
      "\n",
      "Component 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.327e+04, tolerance: 7.687e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.07934\n",
      "Selected features: [  0   1   2   3   4   5   7   8   9  11  12  13  17  18  19  20  21  22\n",
      "  24  25  26  27  29  30  31  32  37  40  41  43  44  46  48  50  51  52\n",
      "  53  56  63  64  65  66  68  69  71  72  73  75  76  77  94  99 107 112\n",
      " 119 121 123 124 125 133 135 140 143 148 165 167 168 170 173 176 177 178\n",
      " 179 183 187 190 191 195 202 218 225 227 229 237 239 240 250 251 254 263\n",
      " 278 283 284 285 288 289 291 293 297 303 304 315 317 319 333 338 339 345\n",
      " 362 365 368 369 371 375 376 384 385 386 387 399 403 404 409 416 417 419\n",
      " 422 429 437 444 446 447 449 452 459 465 466 467 472 475 476 477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.321e+03, tolerance: 6.799e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.97468\n",
      "\n",
      "Component 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.100e+04, tolerance: 5.784e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.08616\n",
      "Selected features: [  0   1   3   8   9  10  11  12  13  17  18  19  23  24  25  27  28  29\n",
      "  30  31  32  42  43  45  46  47  50  51  53  57  58  62  63  64  65  66\n",
      "  67  68  69  70  71  72  73  74  75  76  83  85  89  96  97 100 102 104\n",
      " 105 106 107 108 115 116 127 129 131 132 136 138 139 142 143 146 148 152\n",
      " 163 168 173 175 184 187 199 200 202 206 207 208 213 214 219 220 223 224\n",
      " 229 233 234 235 236 238 239 240 241 249 250 251 252 253 260 261 264 265\n",
      " 286 298 308 309 316 318 319 321 329 330 332 334 337 338 344 349 351 364\n",
      " 369 371 374 394 395 396 397 402 403 420 421 422 425 427 428 434 439 444\n",
      " 445 449 455 459 460 461 462 464 465 467 469 471 472 473 474 476 477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 139.9193654226474, tolerance: 120.08146059106546\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.04399279705285, tolerance: 120.08146059106546\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.129e+03, tolerance: 4.360e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.98697\n",
      "\n",
      "Component 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.875e+03, tolerance: 6.381e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.07133\n",
      "Selected features: [  0   1   2   3   5   6   7  10  11  18  19  20  22  23  25  27  28  29\n",
      "  30  31  32  33  35  36  38  39  40  41  42  43  44  47  48  50  51  52\n",
      "  53  55  57  60  61  62  66  67  68  69  71  72  73  74  76  77  86  92\n",
      "  93  96 104 106 108 119 123 124 125 128 130 132 133 134 142 147 148 150\n",
      " 152 157 160 165 168 171 173 174 175 176 177 178 179 193 194 195 200 201\n",
      " 203 205 207 208 214 217 218 226 228 231 239 240 241 242 244 245 248 249\n",
      " 259 260 266 268 273 274 277 294 295 296 297 303 314 319 323 328 329 330\n",
      " 335 337 338 339 343 344 347 349 352 353 359 369 371 372 373 376 377 384\n",
      " 391 398 407 409 416 420 421 425 428 432 434 436 441 444 445 446 448 449\n",
      " 452 453 459 469 472 474 475 477]\n",
      "Test set R^2 with selected features: 0.97781\n",
      "\n",
      "Component 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 560.7858864385053, tolerance: 418.686713142023\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 605.0852908269953, tolerance: 418.686713142023\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523.8739534187334, tolerance: 418.686713142023\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 455.83763538536004, tolerance: 418.686713142023\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.638e+03, tolerance: 5.306e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.07135\n",
      "Selected features: [  0   1   3   5   6   8   9  10  11  12  14  15  17  18  22  24  27  31\n",
      "  32  36  40  41  42  43  44  46  47  48  49  50  51  52  53  57  60  62\n",
      "  64  65  67  68  69  70  71  72  73  75  85  90  91  92  94 101 103 104\n",
      " 106 108 110 112 113 116 119 122 128 133 138 139 140 142 143 144 145 151\n",
      " 164 166 169 170 174 175 193 197 200 201 204 205 206 207 211 221 225 228\n",
      " 229 232 234 235 236 239 240 243 244 247 248 257 258 264 266 267 270 272\n",
      " 278 280 284 286 294 296 298 299 300 303 309 315 316 319 320 329 330 333\n",
      " 334 337 342 345 348 350 352 353 356 362 364 366 367 368 370 373 374 375\n",
      " 376 378 384 387 388 389 399 404 405 406 408 409 410 413 421 424 426 427\n",
      " 429 430 431 433 438 444 445 448 449 452 453 460 463 464 466 473 476 477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+03, tolerance: 4.187e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.9498\n",
      "\n",
      "Component 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.399e+02, tolerance: 1.378e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.678e+02, tolerance: 2.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.087e+02, tolerance: 2.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.621e+02, tolerance: 2.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.861e+02, tolerance: 2.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+02, tolerance: 2.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.482e+02, tolerance: 2.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.070e+03, tolerance: 5.414e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.06094\n",
      "Selected features: [  0   3   5   6   8   9  10  12  13  19  20  21  22  24  25  27  29  30\n",
      "  31  32  35  36  39  41  43  45  47  48  49  50  51  52  53  62  63  64\n",
      "  65  66  68  69  70  71  72  73  74  75  76  86  87  99 100 101 103 105\n",
      " 106 108 117 123 125 127 129 130 131 137 138 141 144 146 148 153 160 162\n",
      " 166 167 170 175 176 177 200 203 208 209 213 215 217 224 225 226 227 228\n",
      " 229 231 235 237 240 242 243 244 245 247 248 249 253 255 261 264 265 268\n",
      " 278 280 289 296 297 299 300 301 304 309 313 314 318 319 321 322 323 329\n",
      " 331 336 337 339 342 344 347 348 370 371 373 375 395 398 400 404 405 406\n",
      " 407 408 410 414 415 423 429 431 432 433 434 435 436 438 439 440 441 443\n",
      " 445 447 448 449 460 463 464 467 468 474 477 478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.977e+03, tolerance: 4.745e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.94625\n",
      "\n",
      "Component 14\n",
      "Selected alpha: 0.11492\n",
      "Selected features: [  0   1   3   5   7   9  10  13  16  27  28  29  30  31  33  38  42  43\n",
      "  45  46  47  50  53  56  59  60  63  69  71  72  76  84  88  89 100 102\n",
      " 113 115 124 136 137 139 147 149 150 164 167 169 176 182 203 206 209 219\n",
      " 229 232 237 238 241 242 249 250 253 257 264 265 286 287 302 305 309 314\n",
      " 318 319 322 324 329 333 337 338 339 342 349 352 360 364 369 371 373 375\n",
      " 376 378 386 387 401 404 406 409 415 417 423 426 432 444 449 451 455 461\n",
      " 465 470 471 473]\n",
      "Test set R^2 with selected features: 0.97604\n",
      "\n",
      "Component 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.624e+00, tolerance: 1.276e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00315\n",
      "Selected features: [  1  15  17  18  22  23  29  31  32  33  35  36  37  42  51  55  57  58\n",
      "  69  74  75  76  77  78  93  98 102 109 123 129 132 140 145 147 148 152\n",
      " 161 165 170 171 172 173 177 182 189 199 204 205 230 242 250 251 252 253\n",
      " 259 260 267 268 269 271 273 274 275 294 301 302 307 308 310 312 323 327\n",
      " 332 334 338 343 368 370 397 400 430 437 450 453 462 466 473 474 477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.692e+00, tolerance: 1.010e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99462\n",
      "\n",
      "Component 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.804e+00, tolerance: 1.154e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00302\n",
      "Selected features: [  1   5  10  14  15  16  18  21  24  28  30  35  36  38  43  47  50  52\n",
      "  56  57  58  61  74  76  77  78  99 101 114 128 130 145 159 160 165 171\n",
      " 172 175 178 184 213 228 233 234 235 237 238 248 250 262 286 290 291 297\n",
      " 311 320 323 337 338 345 348 351 369 370 371 373 374 378 403 407 418 436\n",
      " 441 448 453 465 470 471]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.263e+00, tolerance: 9.782e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.9967\n",
      "\n",
      "Component 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.361e-01, tolerance: 1.006e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.789e+00, tolerance: 9.845e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00296\n",
      "Selected features: [  4   5   6   9  11  14  17  18  30  34  36  38  49  54  55  57  60  61\n",
      "  63  66  67  73  74  75  76  77  78 108 110 129 138 141 142 159 160 164\n",
      " 165 167 168 207 213 229 233 242 248 252 253 262 264 265 266 270 271 284\n",
      " 285 290 306 309 313 316 317 320 332 334 335 336 338 339 343 345 352 354\n",
      " 363 368 370 376 378 401 402 403 413 434 437 441 442 443 449 453 455]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.386e+00, tolerance: 8.746e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.9914\n",
      "\n",
      "Component 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.464e+00, tolerance: 1.313e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00358\n",
      "Selected features: [  2   4   8  11  15  16  17  18  34  35  36  54  57  62  65  67  69  72\n",
      "  74  76  77  94 100 108 120 121 126 139 142 149 153 169 170 171 172 198\n",
      " 227 233 240 241 242 244 248 259 265 267 269 278 298 303 331 334 335 338\n",
      " 347 350 351 372 374 384 394 396 397 409 412 413 423 440 457 460 463 469\n",
      " 474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.307e+00, tolerance: 1.114e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99588\n",
      "\n",
      "Component 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+01, tolerance: 6.943e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00247\n",
      "Selected features: [  0   5  10  11  12  13  15  16  17  18  26  28  30  34  35  36  37  38\n",
      "  43  44  46  47  48  55  56  58  63  64  66  70  76  77  78  86  97  99\n",
      " 101 106 111 112 132 143 148 153 158 165 167 174 178 184 187 188 189 190\n",
      " 211 224 230 231 233 234 235 237 238 239 240 245 246 248 253 259 260 265\n",
      " 271 272 279 286 288 303 305 307 321 322 323 329 334 336 343 363 366 369\n",
      " 371 390 400 405 407 426 432 436 444 448 450 456 462 468 470 471 472 475\n",
      " 477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.933e+00, tolerance: 5.189e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99551\n",
      "\n",
      "Component 20\n",
      "Selected alpha: 0.13273\n",
      "Selected features: [  3   4   5   7   8   9  10  12  13  17  20  21  22  24  25  26  28  30\n",
      "  31  32  33  34  35  36  40  41  42  43  44  46  47  48  49  50  51  52\n",
      "  53  57  59  60  61  62  63  64  66  67  69  95 101 104 108 110 111 115\n",
      " 116 117 138 143 144 148 151 152 164 168 175 179 191 193 194 196 228 253\n",
      " 259 264 266 267 268 270 271 286 289 301 302 303 321 328 331 332 333 334\n",
      " 337 338 345 346 354 355 368 409 424 425 430 433 444 446 448 457 478]\n",
      "Test set R^2 with selected features: 0.99394\n",
      "\n",
      "Component 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.429e+03, tolerance: 1.423e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.11728\n",
      "Selected features: [  0   2   4   5   7   9  11  12  13  18  20  22  23  24  26  28  29  31\n",
      "  32  33  41  42  43  46  47  48  49  50  51  52  57  59  60  63  64  68\n",
      "  69  73  76  82  84  89  93 113 126 129 131 132 133 138 144 145 150 151\n",
      " 155 159 160 161 169 173 178 184 185 196 198 214 223 226 228 229 239 253\n",
      " 262 264 265 271 286 293 294 302 305 309 323 325 327 344 346 348 362 364\n",
      " 365 368 372 384 388 397 398 399 403 413 414 415 423 426 437 438 439 444\n",
      " 446 449 452 463 468 471]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.320e+03, tolerance: 1.071e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99497\n",
      "\n",
      "Component 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.050e+03, tolerance: 1.588e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.12985\n",
      "Selected features: [  0   1   4   6   7   8   9  11  13  14  18  20  21  22  28  29  30  32\n",
      "  33  34  37  39  42  44  50  54  56  60  62  63  64  65  69  73  75  79\n",
      "  80  81  82  90 101 103 113 116 142 143 147 149 150 151 152 175 186 189\n",
      " 198 223 241 244 245 250 258 264 267 268 269 275 277 279 280 281 283 285\n",
      " 286 294 309 313 318 321 322 325 327 328 338 344 345 346 368 372 373 374\n",
      " 378 384 386 394 404 405 409 414 415 417 418 429 438 442 448 456 457 459\n",
      " 460 465 466 473]\n",
      "Test set R^2 with selected features: 0.99447\n",
      "\n",
      "Component 23\n",
      "Selected alpha: 0.33681\n",
      "Selected features: [  5   6  11  12  13  15  20  21  23  24  25  28  29  31  33  40  43  46\n",
      "  48  51  52  53  61  70  72  73  75  77 104 108 112 121 125 130 136 142\n",
      " 152 158 171 175 194 206 207 268 274 275 299 301 315 318 334 337 338 371\n",
      " 375 376 377 389 401 404 414 423 425 432 434 436 448 449 459 469 470]\n",
      "Test set R^2 with selected features: 0.98574\n",
      "\n",
      "Component 24\n",
      "Selected alpha: 0.08718\n",
      "Selected features: [  1   2   3   4   5   6   7  10  13  24  25  26  28  30  31  32  33  36\n",
      "  38  41  43  44  46  47  49  50  51  52  57  60  61  64  66  68  71  72\n",
      "  73  74  89  91  93  94  95  99 101 107 110 117 121 122 123 130 132 135\n",
      " 139 141 144 152 160 163 174 175 177 178 180 193 197 198 199 203 209 210\n",
      " 217 227 228 234 238 242 243 250 251 253 288 289 291 305 308 309 319 320\n",
      " 323 324 327 328 329 330 331 332 333 338 342 344 346 349 351 361 362 367\n",
      " 373 378 384 398 400 404 405 409 412 415 418 423 428 430 434 435 443 449\n",
      " 452 454 455 456 459 460 461 466 468 473 476]\n",
      "Test set R^2 with selected features: 0.97729\n",
      "\n",
      "Component 25\n",
      "Selected alpha: 0.09808\n",
      "Selected features: [  0   1   3   5   6   8   9  10  11  12  13  20  21  24  27  28  29  30\n",
      "  32  33  34  38  39  40  42  44  45  46  47  48  50  52  53  57  60  61\n",
      "  62  65  68  69  73  79  99 101 104 108 109 114 121 128 129 130 131 140\n",
      " 141 142 143 146 148 149 151 157 158 165 172 175 176 177 201 204 206 207\n",
      " 208 225 226 231 232 233 243 244 246 247 249 252 254 258 264 267 268 278\n",
      " 279 292 293 300 304 305 310 313 314 319 323 333 334 338 343 348 353 361\n",
      " 364 370 374 375 376 378 385 397 398 408 410 414 418 424 432 433 449 456\n",
      " 457 464 472 475]\n",
      "Test set R^2 with selected features: 0.98983\n",
      "\n",
      "Component 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.976e+03, tolerance: 8.274e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.08419\n",
      "Selected features: [  0   3   4   5   6   8   9  10  11  13  14  19  20  21  23  24  28  29\n",
      "  33  35  36  39  40  43  46  47  48  49  50  51  52  53  57  58  60  61\n",
      "  63  64  68  69  70  71  72  73  77  85  86  91 101 103 107 117 125 130\n",
      " 131 136 140 144 149 151 160 161 173 175 176 177 191 201 202 209 212 213\n",
      " 223 227 229 233 234 235 241 249 253 261 263 271 272 273 278 284 285 286\n",
      " 288 291 296 297 299 301 305 306 310 311 313 317 323 325 329 330 338 344\n",
      " 350 352 364 377 389 394 395 399 403 413 418 424 427 428 429 433 434 435\n",
      " 436 444 453 459 460 461 462 463 464 465 466 467 470 473 475]\n",
      "Test set R^2 with selected features: 0.96997\n",
      "\n",
      "Component 27\n",
      "Selected alpha: 0.0933\n",
      "Selected features: [  1   2   3   5   6   9  12  13  16  23  24  26  27  28  29  30  31  33\n",
      "  41  42  43  44  45  47  49  50  57  60  67  68  70  72  73  77  84  88\n",
      "  91 105 106 109 115 116 121 124 129 146 150 155 159 166 175 176 202 203\n",
      " 209 210 213 214 215 223 228 234 260 262 265 266 267 268 269 309 313 315\n",
      " 318 323 324 325 331 332 335 336 337 338 358 360 361 367 368 370 373 376\n",
      " 377 385 386 396 398 401 402 407 408 410 411 418 419 425 426 428 446 451\n",
      " 452 453 464 465 466 469 477]\n",
      "Test set R^2 with selected features: 0.9912\n",
      "\n",
      "Component 28\n",
      "Selected alpha: 0.16651\n",
      "Selected features: [  4   5   7   8   9  10  13  14  25  26  28  29  30  31  32  33  36  43\n",
      "  44  45  47  48  50  53  58  59  60  62  64  66  69  72  73  88  98 101\n",
      " 103 113 125 130 131 144 147 148 175 177 194 208 218 219 224 225 229 235\n",
      " 244 245 261 263 265 267 278 286 299 303 316 318 320 332 338 348 349 351\n",
      " 364 373 374 388 402 403 411 417 429 430 432 449 453 461 462 464]\n",
      "Test set R^2 with selected features: 0.97725\n",
      "\n",
      "Component 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.180e+02, tolerance: 1.934e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.65431\n",
      "Selected features: [  3   6   7   8   9  10  14  18  19  23  25  26  28  31  39  40  43  47\n",
      "  49  51  53  64  65  66  68  69  71  73 109 114 129 143 144 152 153 160\n",
      " 169 175 190 194 195 220 221 224 228 300 304 319 336 345 373 375 433 434\n",
      " 444 448 450 470]\n",
      "Test set R^2 with selected features: 0.90175\n",
      "\n",
      "Component 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.188e+02, tolerance: 4.894e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.07242\n",
      "Selected features: [  3   5   6   7   9  11  12  14  17  19  21  23  24  25  26  28  29  30\n",
      "  31  32  33  35  36  37  40  43  45  46  47  48  49  50  51  52  57  59\n",
      "  61  62  63  64  66  67  68  69  72  73  84  89  90  91  93 101 104 109\n",
      " 114 116 122 124 125 129 132 133 135 138 141 143 146 147 148 159 161 168\n",
      " 169 172 176 178 194 195 200 201 214 222 223 224 226 229 230 231 232 234\n",
      " 236 237 239 245 246 248 249 250 251 258 259 264 265 268 269 271 272 273\n",
      " 277 278 296 303 304 305 317 320 322 323 324 329 330 332 335 336 337 338\n",
      " 340 344 346 348 350 365 370 374 375 376 377 378 394 395 397 398 403 411\n",
      " 412 414 416 419 420 422 423 425 426 429 434 441 446 447 448 449 450 451\n",
      " 453 458 461 462 465 467 468 473 476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.133e+02, tolerance: 4.456e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.95794\n",
      "\n",
      "Component 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.405e+03, tolerance: 5.749e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.09216\n",
      "Selected features: [  0   2   3   5   6   8   9  11  13  14  18  19  20  22  25  27  29  30\n",
      "  31  33  36  37  41  43  45  46  47  48  49  50  51  52  57  58  60  62\n",
      "  63  65  68  69  70  71  72  75  76  81  98  99 100 104 110 113 121 123\n",
      " 124 127 128 130 131 134 135 138 139 140 142 150 152 154 171 173 175 177\n",
      " 190 191 192 195 199 206 211 214 220 222 224 228 231 233 234 238 239 242\n",
      " 247 248 249 253 265 267 268 270 271 272 274 276 284 285 297 307 308 316\n",
      " 318 319 323 332 333 335 336 337 339 340 341 344 345 348 349 370 373 375\n",
      " 376 384 387 388 398 399 404 405 406 414 424 430 431 434 438 440 441 445\n",
      " 446 449 450 451 453 454 458 459 463 466 467 468 471 472 474 477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.710e+02, tolerance: 4.722e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.97876\n",
      "\n",
      "Component 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.338e+03, tolerance: 4.726e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.11354\n",
      "Selected features: [  3   4   5   6   7   9  10  11  12  13  16  19  21  22  23  28  30  31\n",
      "  32  33  42  43  44  45  46  47  49  52  54  55  58  60  61  62  63  64\n",
      "  65  66  68  70  72  73  78  91  94  99 100 101 102 104 105 107 108 111\n",
      " 112 124 125 129 131 133 134 135 136 148 154 174 176 177 178 183 191 194\n",
      " 196 199 203 210 221 224 225 227 232 234 237 239 241 244 247 248 249 251\n",
      " 252 260 261 264 266 267 271 272 275 276 277 281 284 288 298 305 312 314\n",
      " 315 318 319 323 324 331 334 345 347 357 363 369 370 373 384 385 386 390\n",
      " 399 403 404 408 411 416 422 429 435 437 439 440 444 451 452 453 458 459\n",
      " 462 467 472 473]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.177e+02, tolerance: 3.908e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.96109\n",
      "\n",
      "Component 33\n",
      "Selected alpha: 0.14039\n",
      "Selected features: [  2   4   7   8  11  12  13  17  20  21  23  24  25  28  32  33  35  40\n",
      "  44  45  46  47  49  50  51  52  54  60  65  68  70  71  72  73  75  77\n",
      " 106 108 109 121 122 123 128 131 135 137 139 141 144 148 151 160 162 164\n",
      " 167 172 173 174 175 176 177 178 188 196 197 202 211 220 223 226 227 230\n",
      " 233 238 239 240 244 245 253 255 259 268 284 297 304 313 318 319 327 335\n",
      " 344 351 369 385 393 394 400 407 408 413 423 424 434 436 437 440 443 449\n",
      " 456 457 459 461 463 466 467 468 476 478]\n",
      "Test set R^2 with selected features: 0.95175\n",
      "\n",
      "Component 34\n",
      "Selected alpha: 0.14542\n",
      "Selected features: [  0   1   2   3   6   7  11  14  18  21  25  26  27  28  29  31  32  33\n",
      "  35  40  41  48  49  51  52  60  64  65  68  71  72  73  75  79  80  87\n",
      "  93  98  99 108 112 113 118 123 126 127 128 142 143 149 160 169 171 175\n",
      " 176 177 197 205 206 223 225 227 245 248 249 264 265 274 275 280 284 296\n",
      " 309 318 323 329 334 336 339 340 344 348 373 377 388 394 395 398 401 404\n",
      " 405 406 409 414 424 429 436 443 445 448 449 450 453 458 459 460 461 462\n",
      " 463 465 466 467 468 474 475 476]\n",
      "Test set R^2 with selected features: 0.96922\n",
      "\n",
      "Component 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.741e+00, tolerance: 1.694e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00418\n",
      "Selected features: [  2   4  12  17  18  19  25  31  33  35  36  37  38  54  55  58  66  67\n",
      "  69  73  75  76  99 102 110 116 118 125 132 135 150 169 170 171 176 199\n",
      " 200 201 204 208 213 226 233 238 239 245 246 247 248 251 259 267 277 278\n",
      " 310 316 335 338 363 365 369 374 378 414 430 434 450 455 478]\n",
      "Test set R^2 with selected features: 0.99594\n",
      "\n",
      "Component 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.868e+00, tolerance: 1.058e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00312\n",
      "Selected features: [  1   5  12  14  15  17  18  19  30  32  36  37  38  49  54  56  57  58\n",
      "  72  76  78  85  98 103 104 106 118 121 130 151 152 153 156 161 165 169\n",
      " 170 171 172 199 207 208 209 210 226 227 230 233 243 251 259 260 264 267\n",
      " 268 269 277 309 310 312 318 328 329 336 337 339 343 347 359 368 370 388\n",
      " 391 393 397 403 404 409 413 424 427 433 436 437 448 449 453 461 462 463\n",
      " 467 469 478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.612e+00, tolerance: 8.501e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99145\n",
      "\n",
      "Component 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.504e-01, tolerance: 1.367e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.627e-01, tolerance: 4.710e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.561e-01, tolerance: 4.710e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6545153802152797, tolerance: 0.5693993350419464\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8401348720091804, tolerance: 0.5693993350419464\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8741246700282232, tolerance: 0.5693993350419464\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7404401255494335, tolerance: 0.5693993350419464\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8508806358724499, tolerance: 0.6956718025399257\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0094526875073058, tolerance: 0.6956718025399257\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.968620222250351, tolerance: 0.6956718025399257\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.071e+00, tolerance: 8.159e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00288\n",
      "Selected features: [  1   9  13  15  16  19  21  30  31  32  33  34  35  38  45  49  55  56\n",
      "  57  58  63  68  69  71  74  76  77  78  99 105 109 113 126 132 133 134\n",
      " 138 140 143 146 151 154 162 169 170 176 177 181 199 204 208 209 213 225\n",
      " 232 233 237 241 243 248 264 265 266 271 272 273 278 280 286 292 298 316\n",
      " 317 318 320 324 334 336 342 351 364 369 374 375 376 378 383 388 399 402\n",
      " 411 413 422 424 429 430 434 435 459 461 464 466]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.011902564421277, tolerance: 0.6232409349607808\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.995e+00, tolerance: 6.957e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99167\n",
      "\n",
      "Component 38\n",
      "Selected alpha: 0.00298\n",
      "Selected features: [  6  18  19  29  31  33  36  37  38  55  57  62  69  74  75  76  78  98\n",
      " 124 125 129 145 150 152 157 169 170 177 178 189 214 233 240 249 266 268\n",
      " 283 293 300 315 325 336 337 343 348 357 358 371 374 402 410 418 427 428\n",
      " 436 449 454 459 463 470]\n",
      "Test set R^2 with selected features: 0.99848\n",
      "\n",
      "Component 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.378e+00, tolerance: 8.840e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00302\n",
      "Selected features: [  4   5   9  13  16  17  18  19  21  31  34  36  37  38  39  42  54  56\n",
      "  57  58  68  70  74  75  76  78  94 100 101 102 113 135 138 140 160 163\n",
      " 164 169 181 191 193 196 198 203 210 212 224 227 230 232 233 237 238 243\n",
      " 247 257 275 276 277 278 280 296 297 299 303 304 309 316 324 333 334 337\n",
      " 345 361 363 368 369 374 388 393 413 414 418 422 433 439 443 446 453 458\n",
      " 460 464 466 467 469 473]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.839e+00, tolerance: 6.793e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99468\n",
      "\n",
      "Component 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.821e+03, tolerance: 1.981e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.14504\n",
      "Selected features: [  2   3   6   8  11  12  13  21  22  23  24  25  30  32  33  34  36  37\n",
      "  41  43  45  47  49  51  52  54  59  62  64  65  67  68  69  70  73  84\n",
      "  89  91  92  93  99 100 103 108 113 114 123 127 129 140 142 143 146 151\n",
      " 173 174 191 204 208 223 229 235 237 240 244 249 258 267 268 271 272 277\n",
      " 278 279 296 303 309 310 313 314 317 318 321 322 338 344 345 348 351 354\n",
      " 364 370 371 372 373 376 378 388 408 426 429 466 470 473]\n",
      "Test set R^2 with selected features: 0.99534\n",
      "\n",
      "Component 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.914e+03, tolerance: 1.175e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.10225\n",
      "Selected features: [  1   3   4   5   6   7   9  10  12  13  15  18  20  23  25  26  27  28\n",
      "  29  30  31  33  34  36  37  42  44  46  47  48  49  50  51  58  60  64\n",
      "  66  67  70  72  73  75  77  85  88  91  92  99 103 108 119 124 132 133\n",
      " 139 145 153 168 169 170 172 173 174 178 200 201 209 211 212 213 225 227\n",
      " 233 234 242 249 275 276 284 285 294 298 316 318 320 324 325 326 327 333\n",
      " 337 338 343 346 347 348 350 372 373 376 378 384 389 413 415 418 422 425\n",
      " 428 435 439 452 453 459 460 461 464 465 475]\n",
      "Test set R^2 with selected features: 0.98953\n",
      "\n",
      "Component 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.197e+04, tolerance: 1.417e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.11698\n",
      "Selected features: [  1   2   3   8   9  10  11  12  13  19  20  21  24  25  27  29  32  34\n",
      "  37  38  40  43  46  47  48  50  51  52  57  60  61  62  63  65  66  67\n",
      "  68  69  70  72  76  78  93  99 101 108 111 118 119 123 135 137 138 141\n",
      " 142 144 151 152 171 175 176 179 185 191 211 219 224 225 229 237 238 241\n",
      " 243 244 245 246 252 253 259 264 269 270 271 277 279 280 285 289 290 295\n",
      " 296 326 328 329 332 336 337 338 339 347 348 373 394 402 414 428 435 442\n",
      " 443 460 463 465 471 472 474 476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.828e+03, tolerance: 1.093e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.9918\n",
      "\n",
      "Component 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.248e+04, tolerance: 1.026e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.09849\n",
      "Selected features: [  1   2   3   4   5   7  10  11  12  14  18  20  21  22  25  27  29  31\n",
      "  32  37  43  44  46  47  49  50  52  53  59  63  64  65  66  69  70  72\n",
      "  73  74  75  78  79  89  94 103 104 108 112 113 114 115 127 130 132 135\n",
      " 137 144 145 148 161 166 168 169 170 171 174 184 186 200 201 202 209 210\n",
      " 216 223 224 225 227 234 236 239 240 245 246 249 253 270 277 283 284 291\n",
      " 294 301 303 305 317 319 325 327 328 332 333 335 336 342 344 348 352 363\n",
      " 369 372 373 374 376 379 389 399 400 403 404 407 413 424 425 426 431 432\n",
      " 433 435 437 446 448 449 451 453 459 462 465 469 470 476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.827e+03, tolerance: 6.933e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.98997\n",
      "\n",
      "Component 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.207e+03, tolerance: 8.685e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.08921\n",
      "Selected features: [  0   6   8   9  10  11  12  13  14  18  19  20  23  24  25  26  28  29\n",
      "  30  31  32  35  37  38  39  40  43  44  46  47  49  50  52  56  58  60\n",
      "  61  62  63  64  65  68  69  70  71  72  73  77  84  85  94  95  98 104\n",
      " 107 108 110 114 118 124 127 133 134 139 143 145 146 149 150 152 158 165\n",
      " 172 178 180 194 201 205 206 208 215 219 220 224 227 232 234 239 244 246\n",
      " 263 268 275 278 292 299 300 301 307 312 322 324 328 331 333 334 337 341\n",
      " 343 344 347 353 370 375 376 384 386 388 389 390 400 406 408 409 410 412\n",
      " 427 429 430 432 444 448 449 450 452 453 456 460 463 464 465 467 472 474]\n",
      "Test set R^2 with selected features: 0.97906\n",
      "\n",
      "Component 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.269e+03, tolerance: 9.245e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.09397\n",
      "Selected features: [  2   6   7   8   9  11  12  13  21  22  23  24  25  27  28  30  31  32\n",
      "  33  35  36  38  39  41  43  44  45  46  47  48  49  50  51  52  62  63\n",
      "  65  66  68  69  70  79  89  99 107 108 109 113 114 119 123 131 132 133\n",
      " 135 137 143 144 146 148 151 163 164 172 175 177 178 185 195 200 201 204\n",
      " 217 219 223 226 228 230 231 232 233 235 236 238 244 246 274 290 291 292\n",
      " 304 305 307 308 313 315 323 328 331 334 335 337 338 343 347 348 360 361\n",
      " 365 373 374 375 376 378 398 399 403 409 414 418 424 425 426 429 430 431\n",
      " 432 434 439 450 451 454 461 472]\n",
      "Test set R^2 with selected features: 0.98465\n",
      "\n",
      "Component 46\n",
      "Selected alpha: 0.1213\n",
      "Selected features: [  0   1   5   6   8  10  11  12  13  14  15  19  24  27  28  29  30  31\n",
      "  36  39  40  41  43  44  46  50  51  53  55  57  60  65  67  68  70  73\n",
      "  77  88  91  92  93  97  98 106 107 109 129 133 134 158 162 166 168 171\n",
      " 175 176 209 210 214 221 222 223 234 241 251 259 260 265 267 272 274 279\n",
      " 287 305 306 309 314 315 318 323 325 332 336 337 338 346 347 350 369 371\n",
      " 372 375 376 377 387 388 398 410 414 415 418 419 424 425 428 429 436 452\n",
      " 464 465 471 472 475]\n",
      "Test set R^2 with selected features: 0.98988\n",
      "\n",
      "Component 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.787e+03, tolerance: 6.153e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.06949\n",
      "Selected features: [  0   1   3   4   5   6   9  10  12  14  17  18  19  20  21  23  24  25\n",
      "  26  29  31  32  33  34  37  41  42  43  44  45  46  47  48  50  52  53\n",
      "  59  60  61  62  63  64  65  66  68  69  73  74  76  79  89 102 104 107\n",
      " 108 114 116 123 124 125 129 131 134 138 140 143 146 148 152 172 174 175\n",
      " 176 177 178 190 199 200 203 210 216 221 224 225 226 228 230 234 235 236\n",
      " 238 239 241 243 247 248 249 251 252 253 260 261 264 267 270 271 272 276\n",
      " 283 299 303 307 310 311 312 313 314 318 319 321 322 323 327 328 332 333\n",
      " 342 348 352 359 361 369 378 384 385 386 387 388 389 394 395 396 403 404\n",
      " 405 406 408 409 411 414 416 429 433 440 449 452 458 462 463 464 467 472\n",
      " 473 478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.438e+03, tolerance: 4.998e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.98311\n",
      "\n",
      "Component 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.257e+02, tolerance: 3.101e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.970e+02, tolerance: 3.101e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.091e+02, tolerance: 3.101e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.850e+02, tolerance: 3.101e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.24636\n",
      "Selected features: [  2   3   4  10  11  12  13  14  20  21  28  29  30  31  32  33  34  35\n",
      "  40  41  42  43  45  47  49  52  53  57  58  60  61  66  67  68  70  74\n",
      "  89  90  91  92  95  98 104 108 109 111 114 119 129 137 139 143 144 167\n",
      " 168 171 176 199 217 218 224 228 234 235 244 251 265 278 298 318 319 323\n",
      " 331 338 343 344 345 347 349 353 371 377 378 385 388 424 426 429 433 453\n",
      " 455 456 460 461 462 463 465 466 467 470 477 478]\n",
      "Test set R^2 with selected features: 0.95822\n",
      "\n",
      "Component 49\n",
      "Selected alpha: 0.28494\n",
      "Selected features: [  3   5  10  11  12  13  17  18  20  23  24  28  30  31  32  33  34  36\n",
      "  37  40  45  47  50  52  62  63  64  66  67  70  72  73  77  93 101 102\n",
      " 113 129 132 133 159 160 161 199 201 228 229 231 232 237 251 253 272 273\n",
      " 278 323 324 337 341 345 352 365 366 398 403 404 406 408 424 425 426 458\n",
      " 461 463 464 465 466 467 468]\n",
      "Test set R^2 with selected features: 0.97445\n",
      "\n",
      "Component 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.806e+02, tolerance: 6.335e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.0963\n",
      "Selected features: [  4   5   7   8   9  11  12  13  14  19  20  21  23  24  26  27  28  29\n",
      "  30  32  33  34  41  43  45  46  47  48  50  51  52  53  59  62  63  65\n",
      "  67  70  71  73  74  79  82  83  85  91  94 103 104 105 107 109 110 113\n",
      " 115 117 120 123 126 127 129 135 139 141 144 151 153 163 166 168 172 173\n",
      " 174 175 177 188 190 196 197 199 201 202 206 208 227 230 231 236 240 244\n",
      " 246 247 249 251 267 271 289 300 316 323 329 330 334 336 338 340 345 348\n",
      " 350 351 352 353 361 368 378 383 394 396 407 409 411 421 422 429 434 441\n",
      " 442 443 444 449 451 453 454 455 459 460 463 464 465 469]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.474e+02, tolerance: 5.390e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.97103\n",
      "\n",
      "Component 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 175.01033924266085, tolerance: 166.91537207528035\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 170.66536009321862, tolerance: 166.91537207528035\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 210.29851720589795, tolerance: 166.91537207528035\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 263.9516843521269, tolerance: 166.91537207528035\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 312.53833942759957, tolerance: 166.91537207528035\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 296.0763942294725, tolerance: 166.91537207528035\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.274e+03, tolerance: 3.277e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.05829\n",
      "Selected features: [  1   4   5   7   8   9  10  11  12  14  17  19  20  24  25  27  28  29\n",
      "  30  31  33  34  37  42  43  44  45  46  47  48  49  50  52  53  56  57\n",
      "  58  59  62  63  64  65  66  67  68  69  73  75  76  77  85  93  96  98\n",
      " 100 104 105 106 109 111 114 116 120 123 128 129 130 131 133 137 139 141\n",
      " 143 144 146 148 153 157 161 164 165 168 173 174 177 180 186 187 189 194\n",
      " 195 198 203 204 205 207 210 211 213 216 222 223 225 226 228 230 231 233\n",
      " 234 235 236 238 239 240 241 242 243 247 248 250 253 260 261 264 265 266\n",
      " 268 271 289 302 303 308 314 318 319 326 327 330 333 334 335 336 337 338\n",
      " 342 344 345 364 365 368 369 372 374 375 376 377 384 385 386 406 407 408\n",
      " 409 414 420 421 424 425 429 438 444 446 449 451 452 455 467 468 469 473\n",
      " 475 478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.9871891946841, tolerance: 120.71516799783241\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.073e+02, tolerance: 2.346e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.93141\n",
      "\n",
      "Component 52\n",
      "Selected alpha: 0.14714\n",
      "Selected features: [  1   5   6   7   8  10  12  13  23  24  26  27  29  30  31  33  34  37\n",
      "  41  42  43  45  47  50  51  52  53  60  63  64  68  69  70  71  75  76\n",
      "  78  87  88  89  90  98 103 113 123 124 126 127 131 134 135 137 138 141\n",
      " 142 144 154 168 173 177 190 193 208 220 222 224 225 233 234 239 247 249\n",
      " 251 266 267 268 274 294 329 333 336 337 338 339 340 344 348 349 350 351\n",
      " 352 370 372 373 378 408 410 424 430 434 435 436 438 441 442 444 446 450\n",
      " 451 452 455 459 460 462 472 473]\n",
      "Test set R^2 with selected features: 0.98457\n",
      "\n",
      "Component 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.998e+01, tolerance: 4.993e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.139e+02, tolerance: 1.313e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.934e+02, tolerance: 2.148e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.704e+02, tolerance: 2.148e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.813e+02, tolerance: 2.148e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.240e+02, tolerance: 2.148e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.028e+02, tolerance: 2.148e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.944e+02, tolerance: 2.148e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.338e+02, tolerance: 2.148e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.262e+02, tolerance: 2.148e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513.9732118372049, tolerance: 291.03333339414286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 636.7331313337927, tolerance: 291.03333339414286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 614.3191800689237, tolerance: 291.03333339414286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497.9515486848395, tolerance: 291.03333339414286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 360.62815721475636, tolerance: 291.03333339414286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 356.22948061372153, tolerance: 291.03333339414286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 567.5157430361578, tolerance: 291.03333339414286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 902.7204805673682, tolerance: 291.03333339414286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 796.6984385396499, tolerance: 350.6793940227734\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556.1957406964502, tolerance: 350.6793940227734\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 481.24552434374345, tolerance: 350.6793940227734\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518.1681226053333, tolerance: 350.6793940227734\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 537.8223634729075, tolerance: 350.6793940227734\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 449.05771082648425, tolerance: 350.6793940227734\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 359.19838760292623, tolerance: 350.6793940227734\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.011e+02, tolerance: 4.312e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.1332\n",
      "Selected features: [  0   1   2   4   5  11  12  18  23  24  31  32  33  34  35  42  43  44\n",
      "  47  48  49  50  52  53  56  57  58  60  62  63  64  65  66  67  68  69\n",
      "  70  72  73  85  92  94  99 113 115 119 129 130 135 139 141 147 175 176\n",
      " 185 193 198 199 204 206 207 209 222 224 225 226 228 230 234 238 239 240\n",
      " 241 242 243 244 246 249 252 253 266 268 284 288 291 296 298 305 309 320\n",
      " 322 324 342 344 347 351 360 367 368 369 371 373 375 378 384 385 386 387\n",
      " 398 399 404 408 412 417 418 422 426 428 431 443 444 446 452 456 459 461\n",
      " 463 466 472 476 477]\n",
      "Test set R^2 with selected features: 0.92584\n",
      "\n",
      "Component 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.594e+03, tolerance: 5.998e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.08633\n",
      "Selected features: [  3   4   5   6   7   8   9  10  12  13  15  20  23  26  28  29  30  32\n",
      "  37  39  43  46  47  48  49  50  51  52  53  58  60  62  63  66  68  69\n",
      "  70  71  72  73  74  79  85  86  87  88  93  96  97  98 104 109 110 112\n",
      " 113 116 117 122 123 126 127 129 130 132 134 137 138 145 149 154 158 159\n",
      " 168 172 173 178 199 200 210 218 219 221 224 225 230 232 234 236 239 240\n",
      " 243 245 247 249 252 256 261 265 268 270 271 272 274 275 278 284 290 301\n",
      " 303 304 317 320 323 327 332 336 338 343 347 349 350 353 367 370 374 375\n",
      " 376 399 404 408 416 418 439 446 447 454 475]\n",
      "Test set R^2 with selected features: 0.98332\n",
      "\n",
      "Component 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.206e+00, tolerance: 1.676e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00415\n",
      "Selected features: [ 11  12  15  16  18  19  25  31  32  33  35  36  55  57  60  74  75  78\n",
      " 114 116 118 126 138 153 171 172 173 175 176 213 215 233 237 238 244 247\n",
      " 248 251 263 264 265 297 298 299 315 333 346 347 355 369 393 434 435 449\n",
      " 453 458 460 466 474 478]\n",
      "Test set R^2 with selected features: 0.99809\n",
      "\n",
      "Component 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.786e-01, tolerance: 6.400e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.430718198803639, tolerance: 0.8541745015273855\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.765429179675058, tolerance: 0.8541745015273855\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8876603715471134, tolerance: 0.8541745015273855\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.544751658791899, tolerance: 0.9493302210985826\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9534512406206979, tolerance: 0.9493302210985826\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.595e+00, tolerance: 1.061e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12181790117281999, tolerance: 0.08490562363052645\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00306\n",
      "Selected features: [ 13  15  17  18  19  31  32  33  34  37  46  55  56  58  61  63  65  74\n",
      "  75  76  77  78 108 109 113 118 126 129 132 133 138 140 146 151 155 162\n",
      " 169 170 176 179 180 181 194 206 207 208 211 213 219 225 232 233 235 247\n",
      " 248 250 251 262 265 266 270 271 273 277 278 280 284 292 315 317 333 334\n",
      " 336 339 343 351 359 361 364 376 402 411 417 418 424 434 439 444 453 459\n",
      " 460 462 466]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8995341584721892, tolerance: 0.7447549543105388\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9121542329995656, tolerance: 0.7447549543105388\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8911735997392896, tolerance: 0.7447549543105388\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.020813644890758, tolerance: 0.8822326642669891\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5632614111004806, tolerance: 0.8822326642669891\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8213992268337869, tolerance: 0.8822326642669891\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4505413443974664, tolerance: 0.8822326642669891\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.286e+00, tolerance: 9.493e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99304\n",
      "\n",
      "Component 57\n",
      "Selected alpha: 0.00315\n",
      "Selected features: [  6  10  12  13  16  17  19  21  31  34  36  39  45  47  48  49  51  53\n",
      "  55  56  57  58  63  74  78  94 110 125 138 145 146 158 163 165 167 178\n",
      " 184 187 188 199 203 212 229 230 233 234 237 238 240 243 260 261 264 265\n",
      " 266 271 287 299 316 321 322 323 329 334 335 336 343 348 354 364 374 383\n",
      " 393 402 406 413 414 436 444 470 471 472 473 476 477]\n",
      "Test set R^2 with selected features: 0.9967\n",
      "\n",
      "Component 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.636e+00, tolerance: 7.663e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00332\n",
      "Selected features: [  6  15  16  28  29  36  37  38  39  40  43  57  58  62  66  69  76  77\n",
      "  78  87  94  95 101 102 107 120 125 150 153 170 171 172 178 198 199 200\n",
      " 201 214 233 240 254 260 302 309 322 323 324 338 348 374 387 389 400 402\n",
      " 405 409 425 429 444 452 457 460 463 467 470 476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.838e-01, tolerance: 6.092e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99624\n",
      "\n",
      "Component 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.800e+00, tolerance: 8.961e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00436\n",
      "Selected features: [  1  12  14  15  16  17  35  36  39  41  46  51  56  57  58  61  72  74\n",
      "  76  78  87  98 102 103 106 112 114 130 153 170 171 172 175 178 199 224\n",
      " 226 230 247 254 268 269 328 329 337 339 345 348 359 362 368 378 386 387\n",
      " 388 404 419 441 448 449 462 469 472 475]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+00, tolerance: 6.962e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99586\n",
      "\n",
      "Component 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+03, tolerance: 2.373e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.15597\n",
      "Selected features: [  0   1   3   9  10  11  12  20  21  22  28  30  33  36  38  40  42  43\n",
      "  44  50  52  53  60  64  65  67  68  69  70  71  72  74  76  77  78  89\n",
      "  91  99 101 103 105 126 145 146 174 175 176 177 178 193 211 226 235 239\n",
      " 240 241 250 251 252 257 264 271 275 278 286 288 308 323 326 332 333 343\n",
      " 344 353 369 374 379 386 388 404 419 420 421 422 423 426 429 432 437 444\n",
      " 446 448 462 464 465 466 467 469]\n",
      "Test set R^2 with selected features: 0.99655\n",
      "\n",
      "Component 61\n",
      "Selected alpha: 0.32583\n",
      "Selected features: [  2   6   7   8  12  13  14  21  22  24  25  29  31  32  36  41  43  44\n",
      "  47  50  51  52  53  60  61  62  63  64  67  68  70  72  73  75  76  79\n",
      "  89  94 110 130 137 138 139 145 146 157 167 168 174 184 188 199 227 228\n",
      " 240 242 247 248 250 251 256 278 282 297 320 321 324 325 327 328 334 347\n",
      " 362 375 382 384 395 398 417 434 449 457 464 466 468 474]\n",
      "Test set R^2 with selected features: 0.98187\n",
      "\n",
      "Component 62\n",
      "Selected alpha: 0.10462\n",
      "Selected features: [  1   2   3   5   6  10  11  23  24  25  26  30  32  33  38  42  44  45\n",
      "  47  48  51  52  53  59  61  64  65  66  68  70  73  75  89  90  94 101\n",
      " 104 108 110 125 129 130 134 136 139 140 148 151 166 172 192 193 194 198\n",
      " 201 205 208 209 212 218 223 231 232 234 237 238 239 251 253 255 256 263\n",
      " 267 272 273 289 298 301 302 303 304 305 317 318 322 330 335 339 348 358\n",
      " 365 369 370 371 387 388 389 401 421 427 434 440 442 446 458 459 460 464\n",
      " 467 470 471 473 474]\n",
      "Test set R^2 with selected features: 0.99283\n",
      "\n",
      "Component 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.829e+03, tolerance: 9.682e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.09442\n",
      "Selected features: [  0   1   2   3   4   5   7  10  11  12  13  17  18  20  24  26  27  28\n",
      "  29  30  31  32  36  37  42  44  45  47  49  50  51  53  54  60  61  63\n",
      "  66  70  71  72  74  78  87  99 104 111 122 124 129 133 138 140 141 142\n",
      " 144 149 150 167 199 208 224 225 233 234 235 238 239 244 253 264 265 276\n",
      " 281 282 294 296 297 301 311 321 322 323 329 330 331 333 334 346 347 348\n",
      " 352 353 359 360 361 368 370 371 372 373 378 384 394 395 398 412 413 414\n",
      " 430 437 438 447 448 451 452 453 459 460 465 467 471 472 473]\n",
      "Test set R^2 with selected features: 0.99354\n",
      "\n",
      "Component 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.895e+03, tolerance: 8.585e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.08702\n",
      "Selected features: [  0   2   3   5   6  10  11  12  14  16  18  19  21  22  24  26  28  30\n",
      "  31  32  36  41  43  44  45  47  48  49  50  51  52  53  54  58  59  61\n",
      "  62  64  65  66  67  68  71  72  73  76  79  89  90  91  92  96  97 107\n",
      " 108 112 114 119 123 126 131 132 138 139 151 156 159 178 183 184 194 196\n",
      " 204 208 210 216 224 228 232 233 235 238 239 241 243 253 263 270 271 272\n",
      " 291 292 294 296 297 298 299 316 318 321 328 330 331 332 333 336 337 338\n",
      " 344 350 355 363 366 369 374 378 382 399 408 410 414 419 424 438 443 452\n",
      " 453 454 455 461 462 464 468 472 474 478]\n",
      "Test set R^2 with selected features: 0.98824\n",
      "\n",
      "Component 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.922e+03, tolerance: 7.651e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.13929\n",
      "Selected features: [  0   3   4   6   7   9  10  11  12  13  20  22  23  26  28  30  31  32\n",
      "  33  34  37  38  39  41  43  45  47  49  50  51  53  57  59  61  62  64\n",
      "  66  67  70  71  72  75  76  79  85 102 113 115 119 120 128 130 134 138\n",
      " 144 145 146 147 171 173 174 178 180 189 190 191 202 206 207 219 221 228\n",
      " 233 234 235 236 240 241 242 245 249 250 253 271 284 291 297 302 309 310\n",
      " 324 325 327 329 331 333 334 338 349 352 353 360 361 362 370 371 372 384\n",
      " 386 387 403 404 407 409 414 417 418 424 432 448 458 459 463 464 465 466]\n",
      "Test set R^2 with selected features: 0.97804\n",
      "\n",
      "Component 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.543e+03, tolerance: 8.329e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.08485\n",
      "Selected features: [  1   2   3   6   8   9  10  11  12  13  14  24  25  27  29  30  31  32\n",
      "  33  34  36  41  42  43  44  45  46  47  48  49  50  51  52  53  58  62\n",
      "  64  65  66  67  68  69  70  72  83  85  89  94 107 117 122 124 130 136\n",
      " 139 140 144 149 151 165 171 175 176 177 178 179 180 187 188 189 192 194\n",
      " 195 197 199 201 204 205 209 219 223 224 226 227 228 230 231 232 233 234\n",
      " 237 238 239 240 242 243 247 258 262 263 270 273 283 289 294 296 304 307\n",
      " 309 316 318 320 323 329 332 334 336 338 345 347 348 352 356 359 364 367\n",
      " 370 384 389 393 395 397 399 401 408 409 419 420 423 429 430 431 433 445\n",
      " 452 453 461 473 477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.576e+03, tolerance: 6.451e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.97379\n",
      "\n",
      "Component 67\n",
      "Selected alpha: 0.11165\n",
      "Selected features: [  1   3   4   8   9  10  11  13  15  24  26  27  28  29  30  31  32  39\n",
      "  40  41  43  45  48  49  51  53  61  65  67  68  70  72  73  76  79  80\n",
      "  90  93  94 103 110 119 120 121 123 124 126 129 131 136 137 138 139 144\n",
      " 147 149 154 157 165 167 170 171 172 175 179 198 203 208 210 218 223 226\n",
      " 232 239 251 253 259 264 268 278 281 282 295 300 303 314 316 325 326 329\n",
      " 331 336 337 339 346 348 350 352 353 366 369 371 374 376 378 391 393 394\n",
      " 396 399 408 414 416 423 425 427 450 451 452 457 468 470]\n",
      "Test set R^2 with selected features: 0.9897\n",
      "\n",
      "Component 68\n",
      "Selected alpha: 0.09574\n",
      "Selected features: [  2   6   7   9  10  11  12  17  19  25  26  27  28  29  30  31  32  33\n",
      "  41  46  47  48  49  50  51  53  56  60  61  63  64  65  66  71  72  73\n",
      "  76  78  93  97  99 103 104 109 110 112 124 126 132 133 136 140 149 150\n",
      " 151 152 165 167 170 171 172 173 175 176 177 183 184 200 201 204 213 214\n",
      " 215 218 227 228 229 230 231 232 233 235 236 240 250 251 252 263 269 288\n",
      " 299 300 307 308 317 320 321 323 324 335 338 344 348 361 362 394 395 398\n",
      " 401 402 405 414 419 427 434 436 446 454 464 465 466 469 471 472]\n",
      "Test set R^2 with selected features: 0.98172\n",
      "\n",
      "Component 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.721e+03, tolerance: 5.767e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.06951\n",
      "Selected features: [  0   1   2   3   6   7   9  10  11  12  13  17  18  19  20  24  25  26\n",
      "  27  31  32  33  34  37  40  42  44  45  46  47  48  49  51  52  53  54\n",
      "  58  59  61  62  63  66  67  69  70  73  75  76  77  84  93 104 105 106\n",
      " 107 110 124 125 126 129 130 139 142 146 148 149 158 164 173 178 186 188\n",
      " 189 200 201 204 206 209 210 215 217 224 225 227 228 230 235 237 238 242\n",
      " 244 245 248 251 256 261 263 266 267 272 275 282 289 306 307 311 314 317\n",
      " 319 323 326 329 332 334 339 347 351 353 366 367 370 374 375 378 385 393\n",
      " 399 403 406 409 413 424 428 433 434 436 438 442 444 445 450 453 460 461\n",
      " 464 465 470 474 475 476 477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.812e+02, tolerance: 4.729e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.96569\n",
      "\n",
      "Component 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.702e+03, tolerance: 6.930e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.07669\n",
      "Selected features: [  0   1   3   4   5   8   9  10  11  12  13  14  16  17  19  20  21  24\n",
      "  25  26  28  29  30  31  32  33  38  40  42  43  44  45  46  47  48  51\n",
      "  53  60  65  66  69  72  73  76  77  79  82  83  86  87  95  99 100 101\n",
      " 104 105 106 119 120 124 128 129 130 132 133 134 139 146 149 150 152 162\n",
      " 171 172 176 177 178 186 201 204 209 215 224 226 227 228 235 239 246 249\n",
      " 252 269 276 286 295 297 298 299 303 310 311 313 314 318 319 320 321 324\n",
      " 325 331 332 334 336 338 342 343 351 363 369 371 372 374 375 384 388 408\n",
      " 416 418 419 424 425 427 428 429 430 431 435 437 447 448 449 452 453 459\n",
      " 460 461 470 471 473]\n",
      "Test set R^2 with selected features: 0.94569\n",
      "\n",
      "Component 71\n",
      "Selected alpha: 0.16139\n",
      "Selected features: [  0   5   7   9  10  11  13  17  23  28  29  34  39  40  42  48  49  50\n",
      "  51  53  54  61  62  64  65  67  68  69  75  94  95  96  97  98 101 108\n",
      " 114 116 125 143 144 158 159 165 175 177 178 182 183 192 193 209 218 223\n",
      " 229 233 238 241 242 245 249 253 267 268 271 285 296 297 302 303 304 306\n",
      " 307 308 313 316 321 328 331 332 333 334 337 338 339 346 352 358 364 365\n",
      " 372 376 377 389 390 392 414 419 420 421 422 424 425 434 438 446 448 449\n",
      " 456 459 470 476 478]\n",
      "Test set R^2 with selected features: 0.98339\n",
      "\n",
      "Component 72\n",
      "Selected alpha: 0.07379\n",
      "Selected features: [  0   4   5   6   7   9  10  11  12  13  14  23  25  26  27  29  30  31\n",
      "  33  34  37  43  44  46  47  50  51  52  53  54  56  59  60  63  65  68\n",
      "  72  75  76  78  89  90 106 124 126 127 129 135 144 145 149 158 178 179\n",
      " 185 188 190 191 198 199 205 207 213 227 228 229 230 231 234 235 236 240\n",
      " 244 248 250 256 267 268 271 272 274 277 302 303 304 308 315 318 322 323\n",
      " 326 333 335 338 343 347 348 349 352 365 370 371 372 373 377 378 389 391\n",
      " 401 402 407 410 421 422 426 428 439 440 443 447 448 449 453 459 460 464\n",
      " 473 474]\n",
      "Test set R^2 with selected features: 0.9881\n",
      "\n",
      "Component 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.790e+03, tolerance: 6.008e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.06777\n",
      "Selected features: [  0   1   2   4   5   8   9  10  11  12  13  14  21  22  23  24  26  27\n",
      "  29  30  31  32  33  34  38  41  42  43  45  49  52  54  58  61  65  69\n",
      "  70  72  73  74  75  84  98 112 114 119 120 121 123 124 125 127 129 134\n",
      " 136 137 138 139 140 141 144 145 146 149 151 156 169 174 175 176 183 188\n",
      " 191 203 204 209 214 223 225 229 231 232 233 234 235 236 238 240 243 245\n",
      " 247 249 253 264 265 267 268 270 277 285 290 297 299 301 302 309 311 317\n",
      " 323 330 334 336 344 345 346 352 372 374 375 377 385 387 390 409 420 430\n",
      " 431 443 444 445 448 452 454 459 460 461 462 463 464 465 467 468 471]\n",
      "Test set R^2 with selected features: 0.96798\n",
      "\n",
      "Component 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.409e+02, tolerance: 1.795e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+02, tolerance: 1.795e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 382.5799360818055, tolerance: 316.492113687957\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.949e+04, tolerance: 3.682e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.04992\n",
      "Selected features: [  2   4   5   7   8   9  10  11  12  13  14  17  19  22  23  25  26  28\n",
      "  29  30  31  32  33  34  36  37  40  41  42  43  44  46  47  48  49  50\n",
      "  51  52  53  54  58  60  61  62  64  65  66  68  69  70  71  73  74  76\n",
      "  78  86  89  92  94  95  96 104 105 106 107 114 119 123 124 127 130 131\n",
      " 133 134 135 136 138 139 141 142 143 144 145 146 148 150 162 163 169 173\n",
      " 174 175 176 177 184 198 203 204 209 210 213 215 217 221 222 223 226 227\n",
      " 229 230 233 236 241 242 244 245 249 251 253 259 260 261 262 265 266 268\n",
      " 270 272 277 278 281 292 294 295 296 299 300 301 304 305 309 314 318 319\n",
      " 322 324 329 331 332 334 336 337 338 342 349 350 352 356 366 369 371 373\n",
      " 377 386 387 394 399 400 404 406 407 408 415 416 419 420 423 424 429 432\n",
      " 437 438 442 444 445 446 448 449 460 461 462 463 464 465 467 468 469 470]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.022e+03, tolerance: 3.165e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.95719\n",
      "\n",
      "Component 75\n",
      "Selected alpha: 0.00678\n",
      "Selected features: [  6  10  11  15  16  18  24  31  37  39  45  50  51  57  58  59  66  70\n",
      "  75  77  84 101 118 125 146 147 165 167 168 173 174 175 192 208 248 253\n",
      " 255 265 276 297 315 318 326 333 336 338 347 353 354 399 400 401 402 434\n",
      " 449 454 459 462 476]\n",
      "Test set R^2 with selected features: 0.99014\n",
      "\n",
      "Component 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.323e-02, tolerance: 7.088e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.115e-02, tolerance: 7.088e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e+01, tolerance: 9.815e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00297\n",
      "Selected features: [  8  14  15  17  18  19  23  24  35  36  38  44  47  55  56  59  61  64\n",
      "  69  75  76  77 101 102 103 109 128 132 138 140 141 143 159 165 169 170\n",
      " 171 172 176 177 178 199 204 213 215 224 228 230 233 241 251 267 271 278\n",
      " 306 310 313 335 390 399 401 403 407 414 417 427 430 431 450 453 457 459\n",
      " 464 466 467 468 469 471 474 475 476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.245e+01, tolerance: 8.872e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99643\n",
      "\n",
      "Component 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e-01, tolerance: 9.362e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.457e-01, tolerance: 9.362e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e-01, tolerance: 9.362e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.600e-01, tolerance: 9.362e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8929355996589123, tolerance: 0.7829302989383375\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5923551317907183, tolerance: 0.7829302989383375\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.679e+00, tolerance: 8.881e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00262\n",
      "Selected features: [ 15  17  18  21  26  29  31  33  35  36  37  38  39  56  57  58  59  63\n",
      "  65  66  70  75  76  77  78  98 129 140 147 157 159 160 169 174 175 176\n",
      " 193 196 205 212 227 233 241 242 243 245 250 252 259 266 278 293 314 332\n",
      " 334 335 336 346 347 352 360 361 363 368 374 384 387 388 393 395 402 403\n",
      " 407 416 417 427 433 436 439 448 453 457 460 462 463 472 474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.444e+00, tolerance: 7.829e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.99561\n",
      "\n",
      "Component 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.772e-01, tolerance: 1.730e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.118e-01, tolerance: 4.993e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.871e-01, tolerance: 4.993e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.405e-01, tolerance: 4.993e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6195301460909945, tolerance: 0.6172012102254203\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8605774743423158, tolerance: 0.6172012102254203\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9395598782091934, tolerance: 0.6172012102254203\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.904647664632698, tolerance: 0.6172012102254203\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8445671813778759, tolerance: 0.6172012102254203\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.05858888963607, tolerance: 0.7717091389116201\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.024680017789862, tolerance: 0.7717091389116201\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9331625262818086, tolerance: 0.7717091389116201\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8508011258962647, tolerance: 0.7717091389116201\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.176e+00, tolerance: 8.971e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08566741346767959, tolerance: 0.07928079189503358\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09571671012712368, tolerance: 0.07928079189503358\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.11026243385061685, tolerance: 0.07928079189503358\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00274\n",
      "Selected features: [  3   6  14  16  17  18  19  25  34  35  36  39  43  50  53  55  56  58\n",
      "  67  75  76  77  78  90  93 103 115 132 140 142 149 158 164 165 166 168\n",
      " 170 171 180 196 199 218 226 228 230 231 233 237 240 244 250 251 259 268\n",
      " 269 275 280 289 308 311 312 314 318 320 334 335 338 339 345 349 364 366\n",
      " 370 371 377 395 408 410 411 412 419 443 444 450 452 453 456 459 464 466\n",
      " 469 473]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14870434547788136, tolerance: 0.1457931008403157\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14930722555354592, tolerance: 0.1457931008403157\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19185079756683088, tolerance: 0.1457931008403157\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19759970369463176, tolerance: 0.1457931008403157\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.39817847093286574, tolerance: 0.37166924047951777\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3869412916394861, tolerance: 0.37166924047951777\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6664529832548389, tolerance: 0.5507572033017119\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7515059166203688, tolerance: 0.5507572033017119\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7439209857562847, tolerance: 0.5507572033017119\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.705754426606962, tolerance: 0.5507572033017119\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6646427588276751, tolerance: 0.5507572033017119\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6252158801195229, tolerance: 0.5507572033017119\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7265034050669783, tolerance: 0.6671919586957045\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8468280594664215, tolerance: 0.6671919586957045\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8335783640863195, tolerance: 0.6671919586957045\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7899200834078108, tolerance: 0.6671919586957045\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7434146962378989, tolerance: 0.6671919586957045\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.631e+00, tolerance: 7.717e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R^2 with selected features: 0.9949\n",
      "\n",
      "Component 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0846637850561116, tolerance: 0.9245412906188526\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.401e+00, tolerance: 1.159e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.00325\n",
      "Selected features: [  6   8  15  16  17  18  30  31  32  37  38  39  48  50  52  53  55  57\n",
      "  58  59  75  77  78  98  99 100 101 121 130 145 165 170 171 178 199 200\n",
      " 201 225 227 252 268 278 314 315 328 336 339 342 343 358 360 364 365 370\n",
      " 371 393 401 409 413 436 437 439 440 441 443 452 457 459 464]\n",
      "Test set R^2 with selected features: 0.99761\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.929e+00, tolerance: 9.245e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "selected_components_lasso = {comp: {} for comp in comps_order_file['comps']}\n",
    "\n",
    "for comp in selected_components_lasso.keys():\n",
    "    print(f\"Component {comp}\")\n",
    "\n",
    "    X = combined_df.drop(columns=[f'c{comp+1}'])\n",
    "    y = combined_df[f'c{comp+1}']\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Model with CV and fit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    lasso_cv = LassoCV(cv=tscv, random_state=123)\n",
    "    lasso_cv.fit(X_scaled, y)\n",
    "\n",
    "    # Selected alpha \n",
    "    selected_components_lasso[comp]['alpha'] = lasso_cv.alpha_\n",
    "    print(\"Selected alpha:\", round(lasso_cv.alpha_, 5))\n",
    "\n",
    "    # Selected features and filter dataset to check on test set \n",
    "    selected_features = np.where(lasso_cv.coef_ != 0)[0]\n",
    "    selected_components_lasso[comp]['selected_features'] = selected_features\n",
    "    print(\"Selected features:\", selected_features)\n",
    "\n",
    "    split_idx = list(tscv.split(X_scaled))[-1][1][0]\n",
    "    X_train_selected = X_scaled[:split_idx, selected_features]\n",
    "    X_test_selected = X_scaled[split_idx:, selected_features]\n",
    "\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    # Evaluate the model on the test set with selected features\n",
    "    lasso_cv_selected = LassoCV(cv=tscv, random_state=12)\n",
    "    lasso_cv_selected.fit(X_train_selected, y_train)\n",
    "    y_pred_selected = lasso_cv_selected.predict(X_test_selected)\n",
    "    \n",
    "    r2_score_cv = lasso_cv_selected.score(X_test_selected, y_test)\n",
    "\n",
    "    selected_components_lasso[comp]['r2_score'] = r2_score_cv\n",
    "    print(\"Test set R^2 with selected features:\", round(r2_score_cv, 5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'alpha': 0.1327913233321651,\n",
       "  'selected_features': array([  0,   1,   2,   3,   7,   9,  11,  12,  13,  19,  20,  21,  23,\n",
       "          24,  25,  26,  28,  29,  30,  32,  34,  38,  41,  43,  44,  46,\n",
       "          52,  53,  57,  59,  62,  63,  64,  67,  68,  69,  70,  72,  73,\n",
       "          79,  84,  93,  98,  99, 101, 104, 120, 122, 124, 125, 126, 129,\n",
       "         130, 142, 148, 149, 151, 165, 177, 188, 198, 201, 203, 204, 206,\n",
       "         207, 208, 228, 231, 234, 236, 239, 240, 241, 243, 246, 252, 260,\n",
       "         261, 267, 268, 274, 293, 294, 303, 304, 322, 323, 326, 329, 330,\n",
       "         339, 341, 346, 347, 353, 354, 360, 366, 368, 370, 371, 372, 373,\n",
       "         398, 408, 412, 422, 423, 425, 427, 434, 450, 452, 456, 457, 460,\n",
       "         461, 468, 472]),\n",
       "  'r2_score': 0.9968656527522426,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c35',\n",
       "   'c39',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c60',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c80',\n",
       "   'c5_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c2_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c54_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c7_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c22_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c35_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c9_lag_5',\n",
       "   'c13_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c73_lag_5'],\n",
       "  'n_features': 120},\n",
       " 1: {'alpha': 0.12279016314052296,\n",
       "  'selected_features': array([  0,   2,   3,   8,   9,  10,  12,  13,  14,  21,  22,  23,  26,\n",
       "          28,  30,  32,  33,  38,  40,  41,  44,  45,  46,  48,  49,  50,\n",
       "          51,  52,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
       "          69,  72,  73,  77,  84,  88,  93,  96, 102, 103, 114, 119, 126,\n",
       "         135, 136, 139, 140, 142, 144, 145, 149, 152, 154, 157, 162, 165,\n",
       "         166, 167, 171, 174, 178, 183, 189, 190, 196, 197, 206, 208, 209,\n",
       "         223, 229, 231, 232, 233, 234, 235, 237, 238, 244, 253, 255, 279,\n",
       "         289, 291, 294, 299, 301, 303, 304, 313, 314, 318, 319, 320, 324,\n",
       "         325, 328, 329, 331, 332, 333, 339, 345, 346, 349, 362, 365, 366,\n",
       "         370, 373, 374, 375, 376, 386, 400, 402, 408, 413, 415, 420, 428,\n",
       "         436, 446, 450, 459, 464, 472]),\n",
       "  'r2_score': 0.9897708642050852,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c39',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c78',\n",
       "   'c5_lag_1',\n",
       "   'c9_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c75_lag_1',\n",
       "   'c78_lag_1',\n",
       "   'c3_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c7_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c24_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c38_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c5_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c16_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c73_lag_5'],\n",
       "  'n_features': 136},\n",
       " 2: {'alpha': 0.17346144491879928,\n",
       "  'selected_features': array([  0,   2,   4,   5,   6,   7,   8,  10,  11,  20,  23,  25,  27,\n",
       "          28,  29,  30,  31,  32,  33,  39,  43,  47,  48,  50,  52,  57,\n",
       "          58,  60,  62,  63,  65,  67,  69,  70,  72,  73,  88,  89,  99,\n",
       "         102, 115, 116, 119, 123, 126, 145, 146, 148, 159, 164, 166, 172,\n",
       "         183, 184, 185, 187, 192, 193, 195, 216, 217, 227, 228, 233, 239,\n",
       "         240, 243, 246, 247, 248, 250, 252, 262, 271, 272, 278, 284, 288,\n",
       "         294, 296, 304, 326, 330, 331, 334, 338, 346, 349, 363, 366, 369,\n",
       "         375, 384, 399, 402, 426, 428, 432, 433, 453, 464, 472, 478]),\n",
       "  'r2_score': 0.9911650606878992,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c3',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c21',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c40',\n",
       "   'c44',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c9_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c5_lag_2',\n",
       "   'c7_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c24_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c26_lag_2',\n",
       "   'c28_lag_2',\n",
       "   'c33_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c57_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c7_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 103},\n",
       " 3: {'alpha': 0.09373026287826408,\n",
       "  'selected_features': array([  0,   1,   3,   4,   5,   6,   8,   9,  10,  11,  12,  13,  15,\n",
       "          19,  20,  23,  25,  26,  28,  30,  31,  32,  33,  35,  36,  39,\n",
       "          40,  42,  45,  46,  47,  48,  51,  53,  56,  57,  58,  59,  60,\n",
       "          61,  62,  63,  64,  65,  66,  68,  69,  73,  75,  76,  88,  94,\n",
       "          96, 101, 104, 119, 121, 122, 125, 127, 129, 137, 139, 148, 170,\n",
       "         175, 190, 199, 203, 205, 206, 209, 210, 214, 217, 218, 223, 231,\n",
       "         234, 237, 239, 243, 248, 249, 262, 263, 266, 268, 276, 277, 279,\n",
       "         280, 281, 283, 284, 285, 292, 294, 299, 300, 303, 305, 315, 319,\n",
       "         324, 325, 328, 331, 332, 333, 334, 337, 338, 344, 348, 351, 353,\n",
       "         363, 365, 368, 369, 371, 373, 378, 384, 385, 388, 398, 400, 402,\n",
       "         409, 415, 419, 421, 425, 426, 427, 428, 431, 438, 439, 440, 445,\n",
       "         446, 448, 451, 456, 461, 464, 467, 468, 474]),\n",
       "  'r2_score': 0.967823459703237,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c16',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c43',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c9_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c11_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c4_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c42_lag_3',\n",
       "   'c44_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c53_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c5_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c75_lag_5'],\n",
       "  'n_features': 152},\n",
       " 4: {'alpha': 0.09341235064190333,\n",
       "  'selected_features': array([  0,   3,   4,   6,   7,   8,   9,  10,  11,  16,  17,  19,  20,\n",
       "          21,  23,  24,  25,  27,  28,  31,  35,  36,  37,  40,  44,  45,\n",
       "          46,  47,  49,  50,  52,  53,  60,  62,  63,  64,  66,  67,  69,\n",
       "          72,  73,  77,  85,  95,  99, 100, 103, 114, 115, 116, 117, 132,\n",
       "         133, 137, 149, 169, 174, 175, 196, 199, 200, 208, 217, 218, 223,\n",
       "         231, 232, 234, 237, 238, 239, 243, 259, 262, 268, 272, 273, 275,\n",
       "         280, 285, 287, 289, 290, 291, 297, 298, 299, 300, 310, 311, 321,\n",
       "         322, 324, 331, 334, 335, 346, 347, 349, 361, 367, 368, 371, 374,\n",
       "         375, 378, 379, 380, 386, 394, 398, 401, 402, 406, 411, 415, 417,\n",
       "         424, 426, 428, 429, 430, 431, 433, 444, 447, 451, 452, 463, 468,\n",
       "         471, 472, 474, 475]),\n",
       "  'r2_score': 0.9908188861154097,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c32',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c41',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c70',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c78',\n",
       "   'c6_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c10_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c4_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c48_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c51_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c48_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c60_lag_4',\n",
       "   'c61_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c76_lag_5'],\n",
       "  'n_features': 134},\n",
       " 5: {'alpha': 0.08314775878058948,\n",
       "  'selected_features': array([  0,   2,   3,   4,   5,   8,   9,  10,  11,  13,  20,  22,  23,\n",
       "          25,  27,  29,  30,  31,  32,  35,  36,  37,  40,  41,  42,  43,\n",
       "          44,  45,  46,  48,  49,  50,  51,  53,  56,  57,  60,  61,  62,\n",
       "          63,  64,  68,  69,  70,  71,  73,  76,  77,  90,  93,  94,  95,\n",
       "          98, 101, 104, 107, 109, 110, 112, 114, 125, 129, 130, 132, 133,\n",
       "         138, 143, 144, 145, 147, 148, 153, 169, 174, 177, 178, 193, 194,\n",
       "         197, 200, 201, 203, 206, 207, 212, 223, 226, 232, 236, 237, 238,\n",
       "         239, 245, 249, 252, 253, 260, 261, 271, 272, 273, 275, 277, 278,\n",
       "         289, 293, 296, 299, 305, 314, 315, 318, 319, 322, 327, 329, 332,\n",
       "         338, 344, 348, 349, 362, 369, 370, 374, 376, 377, 378, 389, 396,\n",
       "         405, 406, 414, 419, 429, 432, 438, 441, 442, 444, 449, 451, 452,\n",
       "         454, 456, 457, 458, 460, 463, 464, 465, 467, 472]),\n",
       "  'r2_score': 0.9670175827472678,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c14',\n",
       "   'c21',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c74',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c11_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c10_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c38_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c53_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c6_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c54_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c3_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c77_lag_4',\n",
       "   'c6_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c73_lag_5'],\n",
       "  'n_features': 153},\n",
       " 6: {'alpha': 0.14751562346801392,\n",
       "  'selected_features': array([  2,   3,   8,   9,  10,  11,  12,  13,  23,  25,  26,  29,  30,\n",
       "          31,  32,  33,  39,  41,  42,  43,  44,  45,  46,  48,  53,  57,\n",
       "          58,  60,  63,  65,  67,  72,  73,  75,  76,  85,  97, 107, 109,\n",
       "         113, 119, 126, 127, 128, 143, 150, 153, 174, 202, 203, 204, 214,\n",
       "         215, 216, 224, 225, 227, 232, 233, 236, 237, 247, 258, 262, 263,\n",
       "         267, 268, 269, 270, 288, 293, 314, 316, 317, 329, 337, 338, 343,\n",
       "         344, 348, 352, 355, 356, 360, 370, 372, 374, 375, 378, 392, 405,\n",
       "         411, 419, 433, 440, 441, 442, 449, 460, 469, 470, 471, 474, 476]),\n",
       "  'r2_score': 0.990417301269375,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c4',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c40',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c49',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c6_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c15_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c57_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c8_lag_3',\n",
       "   'c19_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c54_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c10_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c36_lag_4',\n",
       "   'c37_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c73_lag_4',\n",
       "   'c6_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 104},\n",
       " 7: {'alpha': 0.1291667561855115,\n",
       "  'selected_features': array([  2,   3,   4,   5,   8,   9,  10,  15,  16,  17,  18,  19,  22,\n",
       "          24,  27,  28,  29,  31,  32,  33,  39,  40,  42,  44,  48,  49,\n",
       "          50,  51,  52,  60,  62,  67,  68,  70,  71,  73,  89,  96,  97,\n",
       "          98, 101, 104, 107, 114, 115, 116, 118, 119, 133, 140, 143, 148,\n",
       "         159, 161, 168, 175, 177, 178, 191, 192, 199, 219, 223, 224, 225,\n",
       "         226, 229, 233, 239, 253, 266, 268, 271, 279, 299, 300, 302, 303,\n",
       "         304, 305, 307, 312, 313, 319, 321, 322, 328, 331, 333, 337, 338,\n",
       "         339, 346, 348, 354, 361, 369, 374, 376, 377, 384, 388, 391, 393,\n",
       "         400, 401, 403, 421, 424, 425, 428, 434, 442, 443, 446, 447, 449,\n",
       "         460, 461, 468, 470, 471]),\n",
       "  'r2_score': 0.9867918485084218,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c23',\n",
       "   'c25',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c43',\n",
       "   'c45',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c74',\n",
       "   'c10_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c2_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c33_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c14_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c73_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c35_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c72_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c2_lag_5',\n",
       "   'c4_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c72_lag_5'],\n",
       "  'n_features': 122},\n",
       " 8: {'alpha': 0.08554770154509082,\n",
       "  'selected_features': array([  0,   1,   2,   4,   7,   8,  10,  12,  16,  18,  23,  27,  28,\n",
       "          30,  31,  32,  33,  35,  37,  38,  39,  41,  43,  44,  46,  47,\n",
       "          48,  49,  50,  52,  59,  60,  63,  64,  66,  67,  68,  69,  70,\n",
       "          72,  73,  76,  80, 101, 103, 110, 111, 114, 117, 119, 120, 123,\n",
       "         124, 129, 131, 143, 144, 147, 148, 167, 171, 177, 178, 192, 195,\n",
       "         199, 206, 214, 217, 218, 219, 222, 223, 226, 227, 231, 232, 234,\n",
       "         235, 237, 239, 244, 249, 253, 258, 259, 265, 269, 271, 275, 279,\n",
       "         281, 282, 288, 289, 295, 299, 300, 303, 309, 314, 318, 323, 324,\n",
       "         325, 329, 336, 338, 339, 340, 343, 344, 346, 348, 349, 350, 358,\n",
       "         364, 369, 373, 374, 377, 378, 387, 388, 391, 396, 409, 411, 413,\n",
       "         414, 416, 421, 423, 424, 425, 427, 432, 435, 441, 444, 447, 453,\n",
       "         455, 456, 457, 460, 461, 462, 466, 469, 471, 472, 475, 477]),\n",
       "  'r2_score': 0.9647388287618636,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c5',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c17',\n",
       "   'c19',\n",
       "   'c24',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c77',\n",
       "   'c1_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c32_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c8_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c33_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c63_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c5_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c19_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c42_lag_3',\n",
       "   'c43_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c56_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c21_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c39_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c72_lag_4',\n",
       "   'c77_lag_4',\n",
       "   'c10_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c76_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 155},\n",
       " 9: {'alpha': 0.07934046307229198,\n",
       "  'selected_features': array([  0,   1,   2,   3,   4,   5,   7,   8,   9,  11,  12,  13,  17,\n",
       "          18,  19,  20,  21,  22,  24,  25,  26,  27,  29,  30,  31,  32,\n",
       "          37,  40,  41,  43,  44,  46,  48,  50,  51,  52,  53,  56,  63,\n",
       "          64,  65,  66,  68,  69,  71,  72,  73,  75,  76,  77,  94,  99,\n",
       "         107, 112, 119, 121, 123, 124, 125, 133, 135, 140, 143, 148, 165,\n",
       "         167, 168, 170, 173, 176, 177, 178, 179, 183, 187, 190, 191, 195,\n",
       "         202, 218, 225, 227, 229, 237, 239, 240, 250, 251, 254, 263, 278,\n",
       "         283, 284, 285, 288, 289, 291, 293, 297, 303, 304, 315, 317, 319,\n",
       "         333, 338, 339, 345, 362, 365, 368, 369, 371, 375, 376, 384, 385,\n",
       "         386, 387, 399, 403, 404, 409, 416, 417, 419, 422, 429, 437, 444,\n",
       "         446, 447, 449, 452, 459, 465, 466, 467, 472, 475, 476, 477]),\n",
       "  'r2_score': 0.9746820502422446,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c38',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c57',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c15_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c20_lag_2',\n",
       "   'c24_lag_2',\n",
       "   'c28_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c15_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c44_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c54_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c14_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c76_lag_5',\n",
       "   'c77_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 142},\n",
       " 10: {'alpha': 0.08615857269125426,\n",
       "  'selected_features': array([  0,   1,   3,   8,   9,  10,  11,  12,  13,  17,  18,  19,  23,\n",
       "          24,  25,  27,  28,  29,  30,  31,  32,  42,  43,  45,  46,  47,\n",
       "          50,  51,  53,  57,  58,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  83,  85,  89,  96,  97, 100,\n",
       "         102, 104, 105, 106, 107, 108, 115, 116, 127, 129, 131, 132, 136,\n",
       "         138, 139, 142, 143, 146, 148, 152, 163, 168, 173, 175, 184, 187,\n",
       "         199, 200, 202, 206, 207, 208, 213, 214, 219, 220, 223, 224, 229,\n",
       "         233, 234, 235, 236, 238, 239, 240, 241, 249, 250, 251, 252, 253,\n",
       "         260, 261, 264, 265, 286, 298, 308, 309, 316, 318, 319, 321, 329,\n",
       "         330, 332, 334, 337, 338, 344, 349, 351, 364, 369, 371, 374, 394,\n",
       "         395, 396, 397, 402, 403, 420, 421, 422, 425, 427, 428, 434, 439,\n",
       "         444, 445, 449, 455, 459, 460, 461, 462, 464, 465, 467, 469, 471,\n",
       "         472, 473, 474, 476, 477]),\n",
       "  'r2_score': 0.9869723140337,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c4',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c4_lag_1',\n",
       "   'c6_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c4_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c28_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c61_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c2_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c77_lag_4',\n",
       "   'c78_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c4_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c77_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 161},\n",
       " 11: {'alpha': 0.07132954804527122,\n",
       "  'selected_features': array([  0,   1,   2,   3,   5,   6,   7,  10,  11,  18,  19,  20,  22,\n",
       "          23,  25,  27,  28,  29,  30,  31,  32,  33,  35,  36,  38,  39,\n",
       "          40,  41,  42,  43,  44,  47,  48,  50,  51,  52,  53,  55,  57,\n",
       "          60,  61,  62,  66,  67,  68,  69,  71,  72,  73,  74,  76,  77,\n",
       "          86,  92,  93,  96, 104, 106, 108, 119, 123, 124, 125, 128, 130,\n",
       "         132, 133, 134, 142, 147, 148, 150, 152, 157, 160, 165, 168, 171,\n",
       "         173, 174, 175, 176, 177, 178, 179, 193, 194, 195, 200, 201, 203,\n",
       "         205, 207, 208, 214, 217, 218, 226, 228, 231, 239, 240, 241, 242,\n",
       "         244, 245, 248, 249, 259, 260, 266, 268, 273, 274, 277, 294, 295,\n",
       "         296, 297, 303, 314, 319, 323, 328, 329, 330, 335, 337, 338, 339,\n",
       "         343, 344, 347, 349, 352, 353, 359, 369, 371, 372, 373, 376, 377,\n",
       "         384, 391, 398, 407, 409, 416, 420, 421, 425, 428, 432, 434, 436,\n",
       "         441, 444, 445, 446, 448, 449, 452, 453, 459, 469, 472, 474, 475,\n",
       "         477]),\n",
       "  'r2_score': 0.9778095993879943,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c56',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c7_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c78_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c20_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c56_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c40_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c72_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c8_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c76_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 170},\n",
       " 12: {'alpha': 0.07135008279559214,\n",
       "  'selected_features': array([  0,   1,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,  17,\n",
       "          18,  22,  24,  27,  31,  32,  36,  40,  41,  42,  43,  44,  46,\n",
       "          47,  48,  49,  50,  51,  52,  53,  57,  60,  62,  64,  65,  67,\n",
       "          68,  69,  70,  71,  72,  73,  75,  85,  90,  91,  92,  94, 101,\n",
       "         103, 104, 106, 108, 110, 112, 113, 116, 119, 122, 128, 133, 138,\n",
       "         139, 140, 142, 143, 144, 145, 151, 164, 166, 169, 170, 174, 175,\n",
       "         193, 197, 200, 201, 204, 205, 206, 207, 211, 221, 225, 228, 229,\n",
       "         232, 234, 235, 236, 239, 240, 243, 244, 247, 248, 257, 258, 264,\n",
       "         266, 267, 270, 272, 278, 280, 284, 286, 294, 296, 298, 299, 300,\n",
       "         303, 309, 315, 316, 319, 320, 329, 330, 333, 334, 337, 342, 345,\n",
       "         348, 350, 352, 353, 356, 362, 364, 366, 367, 368, 370, 373, 374,\n",
       "         375, 376, 378, 384, 387, 388, 389, 399, 404, 405, 406, 408, 409,\n",
       "         410, 413, 421, 424, 426, 427, 429, 430, 431, 433, 438, 444, 445,\n",
       "         448, 449, 452, 453, 460, 463, 464, 466, 473, 476, 477]),\n",
       "  'r2_score': 0.949798876508284,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c15',\n",
       "   'c16',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c23',\n",
       "   'c25',\n",
       "   'c28',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c37',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c6_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c5_lag_2',\n",
       "   'c7_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c38_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c62_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c18_lag_3',\n",
       "   'c19_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c37_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c48_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c5_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c77_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 180},\n",
       " 13: {'alpha': 0.06094403902579924,\n",
       "  'selected_features': array([  0,   3,   5,   6,   8,   9,  10,  12,  13,  19,  20,  21,  22,\n",
       "          24,  25,  27,  29,  30,  31,  32,  35,  36,  39,  41,  43,  45,\n",
       "          47,  48,  49,  50,  51,  52,  53,  62,  63,  64,  65,  66,  68,\n",
       "          69,  70,  71,  72,  73,  74,  75,  76,  86,  87,  99, 100, 101,\n",
       "         103, 105, 106, 108, 117, 123, 125, 127, 129, 130, 131, 137, 138,\n",
       "         141, 144, 146, 148, 153, 160, 162, 166, 167, 170, 175, 176, 177,\n",
       "         200, 203, 208, 209, 213, 215, 217, 224, 225, 226, 227, 228, 229,\n",
       "         231, 235, 237, 240, 242, 243, 244, 245, 247, 248, 249, 253, 255,\n",
       "         261, 264, 265, 268, 278, 280, 289, 296, 297, 299, 300, 301, 304,\n",
       "         309, 313, 314, 318, 319, 321, 322, 323, 329, 331, 336, 337, 339,\n",
       "         342, 344, 347, 348, 370, 371, 373, 375, 395, 398, 400, 404, 405,\n",
       "         406, 407, 408, 410, 414, 415, 423, 429, 431, 432, 433, 434, 435,\n",
       "         436, 438, 439, 440, 441, 443, 445, 447, 448, 449, 460, 463, 464,\n",
       "         467, 468, 474, 477, 478]),\n",
       "  'r2_score': 0.9462454317791397,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c40',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c7_lag_1',\n",
       "   'c8_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c3_lag_2',\n",
       "   'c7_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c16_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c78_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 174},\n",
       " 14: {'alpha': 0.11492333671715986,\n",
       "  'selected_features': array([  0,   1,   3,   5,   7,   9,  10,  13,  16,  27,  28,  29,  30,\n",
       "          31,  33,  38,  42,  43,  45,  46,  47,  50,  53,  56,  59,  60,\n",
       "          63,  69,  71,  72,  76,  84,  88,  89, 100, 102, 113, 115, 124,\n",
       "         136, 137, 139, 147, 149, 150, 164, 167, 169, 176, 182, 203, 206,\n",
       "         209, 219, 229, 232, 237, 238, 241, 242, 249, 250, 253, 257, 264,\n",
       "         265, 286, 287, 302, 305, 309, 314, 318, 319, 322, 324, 329, 333,\n",
       "         337, 338, 339, 342, 349, 352, 360, 364, 369, 371, 373, 375, 376,\n",
       "         378, 386, 387, 401, 404, 406, 409, 415, 417, 423, 426, 432, 444,\n",
       "         449, 451, 455, 461, 465, 470, 471, 473]),\n",
       "  'r2_score': 0.9760381326688999,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c14',\n",
       "   'c17',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c39',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c51',\n",
       "   'c54',\n",
       "   'c57',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c64',\n",
       "   'c70',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c77',\n",
       "   'c5_lag_1',\n",
       "   'c9_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c5_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c23_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c18_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c48_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c3_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 112},\n",
       " 15: {'alpha': 0.0031477977390273887,\n",
       "  'selected_features': array([  1,  15,  17,  18,  22,  23,  29,  31,  32,  33,  35,  36,  37,\n",
       "          42,  51,  55,  57,  58,  69,  74,  75,  76,  77,  78,  93,  98,\n",
       "         102, 109, 123, 129, 132, 140, 145, 147, 148, 152, 161, 165, 170,\n",
       "         171, 172, 173, 177, 182, 189, 199, 204, 205, 230, 242, 250, 251,\n",
       "         252, 253, 259, 260, 267, 268, 269, 271, 273, 274, 275, 294, 301,\n",
       "         302, 307, 308, 310, 312, 323, 327, 332, 334, 338, 343, 368, 370,\n",
       "         397, 400, 430, 437, 450, 453, 462, 466, 473, 474, 477]),\n",
       "  'r2_score': 0.9946207777481135,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c16',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c43',\n",
       "   'c52',\n",
       "   'c56',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c70',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c14_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c2_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c23_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c3_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c73_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c78_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 89},\n",
       " 16: {'alpha': 0.0030173937392148353,\n",
       "  'selected_features': array([  1,   5,  10,  14,  15,  16,  18,  21,  24,  28,  30,  35,  36,\n",
       "          38,  43,  47,  50,  52,  56,  57,  58,  61,  74,  76,  77,  78,\n",
       "          99, 101, 114, 128, 130, 145, 159, 160, 165, 171, 172, 175, 178,\n",
       "         184, 213, 228, 233, 234, 235, 237, 238, 248, 250, 262, 286, 290,\n",
       "         291, 297, 311, 320, 323, 337, 338, 345, 348, 351, 369, 370, 371,\n",
       "         373, 374, 378, 403, 407, 418, 436, 441, 448, 453, 465, 470, 471]),\n",
       "  'r2_score': 0.9966995210104286,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c6',\n",
       "   'c11',\n",
       "   'c15',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c19',\n",
       "   'c22',\n",
       "   'c25',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c39',\n",
       "   'c44',\n",
       "   'c48',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c62',\n",
       "   'c75',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c20_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c9_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c51_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c72_lag_5'],\n",
       "  'n_features': 78},\n",
       " 17: {'alpha': 0.0029580203250783614,\n",
       "  'selected_features': array([  4,   5,   6,   9,  11,  14,  17,  18,  30,  34,  36,  38,  49,\n",
       "          54,  55,  57,  60,  61,  63,  66,  67,  73,  74,  75,  76,  77,\n",
       "          78, 108, 110, 129, 138, 141, 142, 159, 160, 164, 165, 167, 168,\n",
       "         207, 213, 229, 233, 242, 248, 252, 253, 262, 264, 265, 266, 270,\n",
       "         271, 284, 285, 290, 306, 309, 313, 316, 317, 320, 332, 334, 335,\n",
       "         336, 338, 339, 343, 345, 352, 354, 363, 368, 370, 376, 378, 401,\n",
       "         402, 403, 413, 434, 437, 441, 442, 443, 449, 453, 455]),\n",
       "  'r2_score': 0.991404380765771,\n",
       "  'selected_features_names': ['c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c15',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c31',\n",
       "   'c35',\n",
       "   'c37',\n",
       "   'c39',\n",
       "   'c50',\n",
       "   'c55',\n",
       "   'c56',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c64',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c29_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c5_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c3_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c51_lag_3',\n",
       "   'c67_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c35_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c4_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c56_lag_5'],\n",
       "  'n_features': 89},\n",
       " 18: {'alpha': 0.0035821660989753096,\n",
       "  'selected_features': array([  2,   4,   8,  11,  15,  16,  17,  18,  34,  35,  36,  54,  57,\n",
       "          62,  65,  67,  69,  72,  74,  76,  77,  94, 100, 108, 120, 121,\n",
       "         126, 139, 142, 149, 153, 169, 170, 171, 172, 198, 227, 233, 240,\n",
       "         241, 242, 244, 248, 259, 265, 267, 269, 278, 298, 303, 331, 334,\n",
       "         335, 338, 347, 350, 351, 372, 374, 384, 394, 396, 397, 409, 412,\n",
       "         413, 423, 440, 457, 460, 463, 469, 474]),\n",
       "  'r2_score': 0.9958795829598899,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c5',\n",
       "   'c9',\n",
       "   'c12',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c35',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c55',\n",
       "   'c58',\n",
       "   'c63',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c70',\n",
       "   'c73',\n",
       "   'c75',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c15_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c12_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c77_lag_4',\n",
       "   'c78_lag_4',\n",
       "   'c10_lag_5',\n",
       "   'c13_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c75_lag_5'],\n",
       "  'n_features': 73},\n",
       " 19: {'alpha': 0.00246649305638273,\n",
       "  'selected_features': array([  0,   5,  10,  11,  12,  13,  15,  16,  17,  18,  26,  28,  30,\n",
       "          34,  35,  36,  37,  38,  43,  44,  46,  47,  48,  55,  56,  58,\n",
       "          63,  64,  66,  70,  76,  77,  78,  86,  97,  99, 101, 106, 111,\n",
       "         112, 132, 143, 148, 153, 158, 165, 167, 174, 178, 184, 187, 188,\n",
       "         189, 190, 211, 224, 230, 231, 233, 234, 235, 237, 238, 239, 240,\n",
       "         245, 246, 248, 253, 259, 260, 265, 271, 272, 279, 286, 288, 303,\n",
       "         305, 307, 321, 322, 323, 329, 334, 336, 343, 363, 366, 369, 371,\n",
       "         390, 400, 405, 407, 426, 432, 436, 444, 448, 450, 456, 462, 468,\n",
       "         470, 471, 472, 475, 477]),\n",
       "  'r2_score': 0.9955145043052481,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c6',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c35',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c56',\n",
       "   'c57',\n",
       "   'c59',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c71',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c7_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c32_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c28_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c71_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c76_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 109},\n",
       " 20: {'alpha': 0.13273061475309525,\n",
       "  'selected_features': array([  3,   4,   5,   7,   8,   9,  10,  12,  13,  17,  20,  21,  22,\n",
       "          24,  25,  26,  28,  30,  31,  32,  33,  34,  35,  36,  40,  41,\n",
       "          42,  43,  44,  46,  47,  48,  49,  50,  51,  52,  53,  57,  59,\n",
       "          60,  61,  62,  63,  64,  66,  67,  69,  95, 101, 104, 108, 110,\n",
       "         111, 115, 116, 117, 138, 143, 144, 148, 151, 152, 164, 168, 175,\n",
       "         179, 191, 193, 194, 196, 228, 253, 259, 264, 266, 267, 268, 270,\n",
       "         271, 286, 289, 301, 302, 303, 321, 328, 331, 332, 333, 334, 337,\n",
       "         338, 345, 346, 354, 355, 368, 409, 424, 425, 430, 433, 444, 446,\n",
       "         448, 457, 478]),\n",
       "  'r2_score': 0.9939369938849283,\n",
       "  'selected_features_names': ['c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c18',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c70',\n",
       "   'c16_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c32_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c5_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c20_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c14_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c35_lag_4',\n",
       "   'c36_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c10_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 107},\n",
       " 21: {'alpha': 0.11727921168501859,\n",
       "  'selected_features': array([  0,   2,   4,   5,   7,   9,  11,  12,  13,  18,  20,  22,  23,\n",
       "          24,  26,  28,  29,  31,  32,  33,  41,  42,  43,  46,  47,  48,\n",
       "          49,  50,  51,  52,  57,  59,  60,  63,  64,  68,  69,  73,  76,\n",
       "          82,  84,  89,  93, 113, 126, 129, 131, 132, 133, 138, 144, 145,\n",
       "         150, 151, 155, 159, 160, 161, 169, 173, 178, 184, 185, 196, 198,\n",
       "         214, 223, 226, 228, 229, 239, 253, 262, 264, 265, 271, 286, 293,\n",
       "         294, 302, 305, 309, 323, 325, 327, 344, 346, 348, 362, 364, 365,\n",
       "         368, 372, 384, 388, 397, 398, 399, 403, 413, 414, 415, 423, 426,\n",
       "         437, 438, 439, 444, 446, 449, 452, 463, 468, 471]),\n",
       "  'r2_score': 0.9949667158699527,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c3',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c19',\n",
       "   'c21',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c58',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c74',\n",
       "   'c77',\n",
       "   'c3_lag_1',\n",
       "   'c5_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c76_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c2_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c26_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c14_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c54_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c78_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c72_lag_5'],\n",
       "  'n_features': 114},\n",
       " 22: {'alpha': 0.12985144978392962,\n",
       "  'selected_features': array([  0,   1,   4,   6,   7,   8,   9,  11,  13,  14,  18,  20,  21,\n",
       "          22,  28,  29,  30,  32,  33,  34,  37,  39,  42,  44,  50,  54,\n",
       "          56,  60,  62,  63,  64,  65,  69,  73,  75,  79,  80,  81,  82,\n",
       "          90, 101, 103, 113, 116, 142, 143, 147, 149, 150, 151, 152, 175,\n",
       "         186, 189, 198, 223, 241, 244, 245, 250, 258, 264, 267, 268, 269,\n",
       "         275, 277, 279, 280, 281, 283, 285, 286, 294, 309, 313, 318, 321,\n",
       "         322, 325, 327, 328, 338, 344, 345, 346, 368, 372, 373, 374, 378,\n",
       "         384, 386, 394, 404, 405, 409, 414, 415, 417, 418, 429, 438, 442,\n",
       "         448, 456, 457, 459, 460, 465, 466, 473]),\n",
       "  'r2_score': 0.9944728887876084,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c5',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c19',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c40',\n",
       "   'c43',\n",
       "   'c45',\n",
       "   'c51',\n",
       "   'c55',\n",
       "   'c57',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c70',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c80',\n",
       "   'c1_lag_1',\n",
       "   'c2_lag_1',\n",
       "   'c3_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c16_lag_2',\n",
       "   'c27_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c19_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c42_lag_3',\n",
       "   'c44_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c5_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 112},\n",
       " 23: {'alpha': 0.33680539752536476,\n",
       "  'selected_features': array([  5,   6,  11,  12,  13,  15,  20,  21,  23,  24,  25,  28,  29,\n",
       "          31,  33,  40,  43,  46,  48,  51,  52,  53,  61,  70,  72,  73,\n",
       "          75,  77, 104, 108, 112, 121, 125, 130, 136, 142, 152, 158, 171,\n",
       "         175, 194, 206, 207, 268, 274, 275, 299, 301, 315, 318, 334, 337,\n",
       "         338, 371, 375, 376, 377, 389, 401, 404, 414, 423, 425, 432, 434,\n",
       "         436, 448, 449, 459, 469, 470]),\n",
       "  'r2_score': 0.9857395861956826,\n",
       "  'selected_features_names': ['c6',\n",
       "   'c7',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c16',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c41',\n",
       "   'c44',\n",
       "   'c47',\n",
       "   'c49',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c62',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c78',\n",
       "   'c25_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c12_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c29_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c15_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c71_lag_5'],\n",
       "  'n_features': 71},\n",
       " 24: {'alpha': 0.087177608265536,\n",
       "  'selected_features': array([  1,   2,   3,   4,   5,   6,   7,  10,  13,  24,  25,  26,  28,\n",
       "          30,  31,  32,  33,  36,  38,  41,  43,  44,  46,  47,  49,  50,\n",
       "          51,  52,  57,  60,  61,  64,  66,  68,  71,  72,  73,  74,  89,\n",
       "          91,  93,  94,  95,  99, 101, 107, 110, 117, 121, 122, 123, 130,\n",
       "         132, 135, 139, 141, 144, 152, 160, 163, 174, 175, 177, 178, 180,\n",
       "         193, 197, 198, 199, 203, 209, 210, 217, 227, 228, 234, 238, 242,\n",
       "         243, 250, 251, 253, 288, 289, 291, 305, 308, 309, 319, 320, 323,\n",
       "         324, 327, 328, 329, 330, 331, 332, 333, 338, 342, 344, 346, 349,\n",
       "         351, 361, 362, 367, 373, 378, 384, 398, 400, 404, 405, 409, 412,\n",
       "         415, 418, 423, 428, 430, 434, 435, 443, 449, 452, 454, 455, 456,\n",
       "         459, 460, 461, 466, 468, 473, 476]),\n",
       "  'r2_score': 0.9772883856993602,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c11',\n",
       "   'c14',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c37',\n",
       "   'c39',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c10_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c4_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c21_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c38_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c3_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c48_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c13_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 137},\n",
       " 25: {'alpha': 0.09808402145733061,\n",
       "  'selected_features': array([  0,   1,   3,   5,   6,   8,   9,  10,  11,  12,  13,  20,  21,\n",
       "          24,  27,  28,  29,  30,  32,  33,  34,  38,  39,  40,  42,  44,\n",
       "          45,  46,  47,  48,  50,  52,  53,  57,  60,  61,  62,  65,  68,\n",
       "          69,  73,  79,  99, 101, 104, 108, 109, 114, 121, 128, 129, 130,\n",
       "         131, 140, 141, 142, 143, 146, 148, 149, 151, 157, 158, 165, 172,\n",
       "         175, 176, 177, 201, 204, 206, 207, 208, 225, 226, 231, 232, 233,\n",
       "         243, 244, 246, 247, 249, 252, 254, 258, 264, 267, 268, 278, 279,\n",
       "         292, 293, 300, 304, 305, 310, 313, 314, 319, 323, 333, 334, 338,\n",
       "         343, 348, 353, 361, 364, 370, 374, 375, 376, 378, 385, 397, 398,\n",
       "         408, 410, 414, 418, 424, 432, 433, 449, 456, 457, 464, 472, 475]),\n",
       "  'r2_score': 0.9898295940955737,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c25',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c43',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c66',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c74',\n",
       "   'c80',\n",
       "   'c20_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c78_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c4_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c15_lag_3',\n",
       "   'c19_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c53_lag_3',\n",
       "   'c54_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c78_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c9_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c76_lag_5'],\n",
       "  'n_features': 130},\n",
       " 26: {'alpha': 0.08418702697997865,\n",
       "  'selected_features': array([  0,   3,   4,   5,   6,   8,   9,  10,  11,  13,  14,  19,  20,\n",
       "          21,  23,  24,  28,  29,  33,  35,  36,  39,  40,  43,  46,  47,\n",
       "          48,  49,  50,  51,  52,  53,  57,  58,  60,  61,  63,  64,  68,\n",
       "          69,  70,  71,  72,  73,  77,  85,  86,  91, 101, 103, 107, 117,\n",
       "         125, 130, 131, 136, 140, 144, 149, 151, 160, 161, 173, 175, 176,\n",
       "         177, 191, 201, 202, 209, 212, 213, 223, 227, 229, 233, 234, 235,\n",
       "         241, 249, 253, 261, 263, 271, 272, 273, 278, 284, 285, 286, 288,\n",
       "         291, 296, 297, 299, 301, 305, 306, 310, 311, 313, 317, 323, 325,\n",
       "         329, 330, 338, 344, 350, 352, 364, 377, 389, 394, 395, 399, 403,\n",
       "         413, 418, 424, 427, 428, 429, 433, 434, 435, 436, 444, 453, 459,\n",
       "         460, 461, 462, 463, 464, 465, 466, 467, 470, 473, 475]),\n",
       "  'r2_score': 0.9699717571113137,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c44',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c78',\n",
       "   'c6_lag_1',\n",
       "   'c7_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c2_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c53_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c67_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c76_lag_5'],\n",
       "  'n_features': 141},\n",
       " 27: {'alpha': 0.09329739900554447,\n",
       "  'selected_features': array([  1,   2,   3,   5,   6,   9,  12,  13,  16,  23,  24,  26,  27,\n",
       "          28,  29,  30,  31,  33,  41,  42,  43,  44,  45,  47,  49,  50,\n",
       "          57,  60,  67,  68,  70,  72,  73,  77,  84,  88,  91, 105, 106,\n",
       "         109, 115, 116, 121, 124, 129, 146, 150, 155, 159, 166, 175, 176,\n",
       "         202, 203, 209, 210, 213, 214, 215, 223, 228, 234, 260, 262, 265,\n",
       "         266, 267, 268, 269, 309, 313, 315, 318, 323, 324, 325, 331, 332,\n",
       "         335, 336, 337, 338, 358, 360, 361, 367, 368, 370, 373, 376, 377,\n",
       "         385, 386, 396, 398, 401, 402, 407, 408, 410, 411, 418, 419, 425,\n",
       "         426, 428, 446, 451, 452, 453, 464, 465, 466, 469, 477]),\n",
       "  'r2_score': 0.991200449451508,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c10',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c17',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c78',\n",
       "   'c5_lag_1',\n",
       "   'c9_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c76_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c7_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c21_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c39_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c48_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c77_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 115},\n",
       " 28: {'alpha': 0.1665148737260208,\n",
       "  'selected_features': array([  4,   5,   7,   8,   9,  10,  13,  14,  25,  26,  28,  29,  30,\n",
       "          31,  32,  33,  36,  43,  44,  45,  47,  48,  50,  53,  58,  59,\n",
       "          60,  62,  64,  66,  69,  72,  73,  88,  98, 101, 103, 113, 125,\n",
       "         130, 131, 144, 147, 148, 175, 177, 194, 208, 218, 219, 224, 225,\n",
       "         229, 235, 244, 245, 261, 263, 265, 267, 278, 286, 299, 303, 316,\n",
       "         318, 320, 332, 338, 348, 349, 351, 364, 373, 374, 388, 402, 403,\n",
       "         411, 417, 429, 430, 432, 449, 453, 461, 462, 464]),\n",
       "  'r2_score': 0.9772486531539346,\n",
       "  'selected_features_names': ['c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c37',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c54',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c70',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c9_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c16_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c5_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c4_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c65_lag_5'],\n",
       "  'n_features': 88},\n",
       " 29: {'alpha': 0.6543134777205294,\n",
       "  'selected_features': array([  3,   6,   7,   8,   9,  10,  14,  18,  19,  23,  25,  26,  28,\n",
       "          31,  39,  40,  43,  47,  49,  51,  53,  64,  65,  66,  68,  69,\n",
       "          71,  73, 109, 114, 129, 143, 144, 152, 153, 160, 169, 175, 190,\n",
       "         194, 195, 220, 221, 224, 228, 300, 304, 319, 336, 345, 373, 375,\n",
       "         433, 434, 444, 448, 450, 470]),\n",
       "  'r2_score': 0.9017536636145416,\n",
       "  'selected_features_names': ['c4',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c15',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c32',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c44',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c72',\n",
       "   'c74',\n",
       "   'c30_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c61_lag_2',\n",
       "   'c62_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c61_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c17_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c34_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c71_lag_5'],\n",
       "  'n_features': 58},\n",
       " 30: {'alpha': 0.07242301976405711,\n",
       "  'selected_features': array([  3,   5,   6,   7,   9,  11,  12,  14,  17,  19,  21,  23,  24,\n",
       "          25,  26,  28,  29,  30,  31,  32,  33,  35,  36,  37,  40,  43,\n",
       "          45,  46,  47,  48,  49,  50,  51,  52,  57,  59,  61,  62,  63,\n",
       "          64,  66,  67,  68,  69,  72,  73,  84,  89,  90,  91,  93, 101,\n",
       "         104, 109, 114, 116, 122, 124, 125, 129, 132, 133, 135, 138, 141,\n",
       "         143, 146, 147, 148, 159, 161, 168, 169, 172, 176, 178, 194, 195,\n",
       "         200, 201, 214, 222, 223, 224, 226, 229, 230, 231, 232, 234, 236,\n",
       "         237, 239, 245, 246, 248, 249, 250, 251, 258, 259, 264, 265, 268,\n",
       "         269, 271, 272, 273, 277, 278, 296, 303, 304, 305, 317, 320, 322,\n",
       "         323, 324, 329, 330, 332, 335, 336, 337, 338, 340, 344, 346, 348,\n",
       "         350, 365, 370, 374, 375, 376, 377, 378, 394, 395, 397, 398, 403,\n",
       "         411, 412, 414, 416, 419, 420, 422, 423, 425, 426, 429, 434, 441,\n",
       "         446, 447, 448, 449, 450, 451, 453, 458, 461, 462, 465, 467, 468,\n",
       "         473, 476]),\n",
       "  'r2_score': 0.9579389529640889,\n",
       "  'selected_features_names': ['c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c15',\n",
       "   'c18',\n",
       "   'c20',\n",
       "   'c22',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c41',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c58',\n",
       "   'c60',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c5_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c2_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c63_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c6_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c19_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c21_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c78_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c13_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 171},\n",
       " 31: {'alpha': 0.09216313151676044,\n",
       "  'selected_features': array([  0,   2,   3,   5,   6,   8,   9,  11,  13,  14,  18,  19,  20,\n",
       "          22,  25,  27,  29,  30,  31,  33,  36,  37,  41,  43,  45,  46,\n",
       "          47,  48,  49,  50,  51,  52,  57,  58,  60,  62,  63,  65,  68,\n",
       "          69,  70,  71,  72,  75,  76,  81,  98,  99, 100, 104, 110, 113,\n",
       "         121, 123, 124, 127, 128, 130, 131, 134, 135, 138, 139, 140, 142,\n",
       "         150, 152, 154, 171, 173, 175, 177, 190, 191, 192, 195, 199, 206,\n",
       "         211, 214, 220, 222, 224, 228, 231, 233, 234, 238, 239, 242, 247,\n",
       "         248, 249, 253, 265, 267, 268, 270, 271, 272, 274, 276, 284, 285,\n",
       "         297, 307, 308, 316, 318, 319, 323, 332, 333, 335, 336, 337, 339,\n",
       "         340, 341, 344, 345, 348, 349, 370, 373, 375, 376, 384, 387, 388,\n",
       "         398, 399, 404, 405, 406, 414, 424, 430, 431, 434, 438, 440, 441,\n",
       "         445, 446, 449, 450, 451, 453, 454, 458, 459, 463, 466, 467, 468,\n",
       "         471, 472, 474, 477]),\n",
       "  'r2_score': 0.9787579122068525,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c23',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c2_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c75_lag_1',\n",
       "   'c12_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c33_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c61_lag_2',\n",
       "   'c63_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c3_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c21_lag_4',\n",
       "   'c22_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c5_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 160},\n",
       " 32: {'alpha': 0.11353721586694641,\n",
       "  'selected_features': array([  3,   4,   5,   6,   7,   9,  10,  11,  12,  13,  16,  19,  21,\n",
       "          22,  23,  28,  30,  31,  32,  33,  42,  43,  44,  45,  46,  47,\n",
       "          49,  52,  54,  55,  58,  60,  61,  62,  63,  64,  65,  66,  68,\n",
       "          70,  72,  73,  78,  91,  94,  99, 100, 101, 102, 104, 105, 107,\n",
       "         108, 111, 112, 124, 125, 129, 131, 133, 134, 135, 136, 148, 154,\n",
       "         174, 176, 177, 178, 183, 191, 194, 196, 199, 203, 210, 221, 224,\n",
       "         225, 227, 232, 234, 237, 239, 241, 244, 247, 248, 249, 251, 252,\n",
       "         260, 261, 264, 266, 267, 271, 272, 275, 276, 277, 281, 284, 288,\n",
       "         298, 305, 312, 314, 315, 318, 319, 323, 324, 331, 334, 345, 347,\n",
       "         357, 363, 369, 370, 373, 384, 385, 386, 390, 399, 403, 404, 408,\n",
       "         411, 416, 422, 429, 435, 437, 439, 440, 444, 451, 452, 453, 458,\n",
       "         459, 462, 467, 472, 473]),\n",
       "  'r2_score': 0.9610877935724401,\n",
       "  'selected_features_names': ['c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c17',\n",
       "   'c20',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c53',\n",
       "   'c55',\n",
       "   'c56',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c79',\n",
       "   'c12_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c32_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c75_lag_1',\n",
       "   'c15_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c24_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c62_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c42_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c73_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c38_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c71_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 148},\n",
       " 33: {'alpha': 0.1403911444768052,\n",
       "  'selected_features': array([  2,   4,   7,   8,  11,  12,  13,  17,  20,  21,  23,  24,  25,\n",
       "          28,  32,  33,  35,  40,  44,  45,  46,  47,  49,  50,  51,  52,\n",
       "          54,  60,  65,  68,  70,  71,  72,  73,  75,  77, 106, 108, 109,\n",
       "         121, 122, 123, 128, 131, 135, 137, 139, 141, 144, 148, 151, 160,\n",
       "         162, 164, 167, 172, 173, 174, 175, 176, 177, 178, 188, 196, 197,\n",
       "         202, 211, 220, 223, 226, 227, 230, 233, 238, 239, 240, 244, 245,\n",
       "         253, 255, 259, 268, 284, 297, 304, 313, 318, 319, 327, 335, 344,\n",
       "         351, 369, 385, 393, 394, 400, 407, 408, 413, 423, 424, 434, 436,\n",
       "         437, 440, 443, 449, 456, 457, 459, 461, 463, 466, 467, 468, 476,\n",
       "         478]),\n",
       "  'r2_score': 0.951753334554505,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c5',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c18',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c29',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c41',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c55',\n",
       "   'c61',\n",
       "   'c66',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c78',\n",
       "   'c27_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c3_lag_2',\n",
       "   'c5_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c38_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c61_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c16_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c8_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c77_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 118},\n",
       " 34: {'alpha': 0.14542003704069872,\n",
       "  'selected_features': array([  0,   1,   2,   3,   6,   7,  11,  14,  18,  21,  25,  26,  27,\n",
       "          28,  29,  31,  32,  33,  35,  40,  41,  48,  49,  51,  52,  60,\n",
       "          64,  65,  68,  71,  72,  73,  75,  79,  80,  87,  93,  98,  99,\n",
       "         108, 112, 113, 118, 123, 126, 127, 128, 142, 143, 149, 160, 169,\n",
       "         171, 175, 176, 177, 197, 205, 206, 223, 225, 227, 245, 248, 249,\n",
       "         264, 265, 274, 275, 280, 284, 296, 309, 318, 323, 329, 334, 336,\n",
       "         339, 340, 344, 348, 373, 377, 388, 394, 395, 398, 401, 404, 405,\n",
       "         406, 409, 414, 424, 429, 436, 443, 445, 448, 449, 450, 453, 458,\n",
       "         459, 460, 461, 462, 463, 465, 466, 467, 468, 474, 475, 476]),\n",
       "  'r2_score': 0.9692186883861885,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c12',\n",
       "   'c15',\n",
       "   'c19',\n",
       "   'c22',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c61',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c69',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c80',\n",
       "   'c1_lag_1',\n",
       "   'c8_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c38_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c6_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c21_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c76_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 116},\n",
       " 35: {'alpha': 0.00417751813881266,\n",
       "  'selected_features': array([  2,   4,  12,  17,  18,  19,  25,  31,  33,  35,  36,  37,  38,\n",
       "          54,  55,  58,  66,  67,  69,  73,  75,  76,  99, 102, 110, 116,\n",
       "         118, 125, 132, 135, 150, 169, 170, 171, 176, 199, 200, 201, 204,\n",
       "         208, 213, 226, 233, 238, 239, 245, 246, 247, 248, 251, 259, 267,\n",
       "         277, 278, 310, 316, 335, 338, 363, 365, 369, 374, 378, 414, 430,\n",
       "         434, 450, 455, 478]),\n",
       "  'r2_score': 0.9959386145202026,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c5',\n",
       "   'c13',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c26',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c55',\n",
       "   'c56',\n",
       "   'c59',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c70',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c20_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c6_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c16_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c15_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 69},\n",
       " 36: {'alpha': 0.0031159556398684066,\n",
       "  'selected_features': array([  1,   5,  12,  14,  15,  17,  18,  19,  30,  32,  36,  37,  38,\n",
       "          49,  54,  56,  57,  58,  72,  76,  78,  85,  98, 103, 104, 106,\n",
       "         118, 121, 130, 151, 152, 153, 156, 161, 165, 169, 170, 171, 172,\n",
       "         199, 207, 208, 209, 210, 226, 227, 230, 233, 243, 251, 259, 260,\n",
       "         264, 267, 268, 269, 277, 309, 310, 312, 318, 328, 329, 336, 337,\n",
       "         339, 343, 347, 359, 368, 370, 388, 391, 393, 397, 403, 404, 409,\n",
       "         413, 424, 427, 433, 436, 437, 448, 449, 453, 461, 462, 463, 467,\n",
       "         469, 478]),\n",
       "  'r2_score': 0.9914469954699775,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c6',\n",
       "   'c13',\n",
       "   'c15',\n",
       "   'c16',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c50',\n",
       "   'c55',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c73',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c6_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c77_lag_1',\n",
       "   'c2_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c4_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c73_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c9_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c40_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c72_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c78_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 93},\n",
       " 37: {'alpha': 0.002875673297992049,\n",
       "  'selected_features': array([  1,   9,  13,  15,  16,  19,  21,  30,  31,  32,  33,  34,  35,\n",
       "          38,  45,  49,  55,  56,  57,  58,  63,  68,  69,  71,  74,  76,\n",
       "          77,  78,  99, 105, 109, 113, 126, 132, 133, 134, 138, 140, 143,\n",
       "         146, 151, 154, 162, 169, 170, 176, 177, 181, 199, 204, 208, 209,\n",
       "         213, 225, 232, 233, 237, 241, 243, 248, 264, 265, 266, 271, 272,\n",
       "         273, 278, 280, 286, 292, 298, 316, 317, 318, 320, 324, 334, 336,\n",
       "         342, 351, 364, 369, 374, 375, 376, 378, 383, 388, 399, 402, 411,\n",
       "         413, 422, 424, 429, 430, 434, 435, 459, 461, 464, 466]),\n",
       "  'r2_score': 0.9916688878281635,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c10',\n",
       "   'c14',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c20',\n",
       "   'c22',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c36',\n",
       "   'c39',\n",
       "   'c46',\n",
       "   'c50',\n",
       "   'c56',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c64',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c72',\n",
       "   'c75',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c20_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c75_lag_1',\n",
       "   'c3_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c22_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c53_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c64_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c67_lag_5'],\n",
       "  'n_features': 102},\n",
       " 38: {'alpha': 0.0029756706806737324,\n",
       "  'selected_features': array([  6,  18,  19,  29,  31,  33,  36,  37,  38,  55,  57,  62,  69,\n",
       "          74,  75,  76,  78,  98, 124, 125, 129, 145, 150, 152, 157, 169,\n",
       "         170, 177, 178, 189, 214, 233, 240, 249, 266, 268, 283, 293, 300,\n",
       "         315, 325, 336, 337, 343, 348, 357, 358, 371, 374, 402, 410, 418,\n",
       "         427, 428, 436, 449, 454, 459, 463, 470]),\n",
       "  'r2_score': 0.9984798160398365,\n",
       "  'selected_features_names': ['c7',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c56',\n",
       "   'c58',\n",
       "   'c63',\n",
       "   'c70',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c19_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c78_lag_1',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c44_lag_3',\n",
       "   'c54_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c6_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c38_lag_4',\n",
       "   'c39_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c71_lag_5'],\n",
       "  'n_features': 60},\n",
       " 39: {'alpha': 0.003016597568285179,\n",
       "  'selected_features': array([  4,   5,   9,  13,  16,  17,  18,  19,  21,  31,  34,  36,  37,\n",
       "          38,  39,  42,  54,  56,  57,  58,  68,  70,  74,  75,  76,  78,\n",
       "          94, 100, 101, 102, 113, 135, 138, 140, 160, 163, 164, 169, 181,\n",
       "         191, 193, 196, 198, 203, 210, 212, 224, 227, 230, 232, 233, 237,\n",
       "         238, 243, 247, 257, 275, 276, 277, 278, 280, 296, 297, 299, 303,\n",
       "         304, 309, 316, 324, 333, 334, 337, 345, 361, 363, 368, 369, 374,\n",
       "         388, 393, 413, 414, 418, 422, 433, 439, 443, 446, 453, 458, 460,\n",
       "         464, 466, 467, 469, 473]),\n",
       "  'r2_score': 0.9946793557735707,\n",
       "  'selected_features_names': ['c5',\n",
       "   'c6',\n",
       "   'c10',\n",
       "   'c14',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c22',\n",
       "   'c32',\n",
       "   'c35',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c43',\n",
       "   'c55',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c15_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c4_lag_2',\n",
       "   'c5_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c22_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c53_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c4_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c18_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c5_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c14_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 96},\n",
       " 40: {'alpha': 0.1450416069244584,\n",
       "  'selected_features': array([  2,   3,   6,   8,  11,  12,  13,  21,  22,  23,  24,  25,  30,\n",
       "          32,  33,  34,  36,  37,  41,  43,  45,  47,  49,  51,  52,  54,\n",
       "          59,  62,  64,  65,  67,  68,  69,  70,  73,  84,  89,  91,  92,\n",
       "          93,  99, 100, 103, 108, 113, 114, 123, 127, 129, 140, 142, 143,\n",
       "         146, 151, 173, 174, 191, 204, 208, 223, 229, 235, 237, 240, 244,\n",
       "         249, 258, 267, 268, 271, 272, 277, 278, 279, 296, 303, 309, 310,\n",
       "         313, 314, 317, 318, 321, 322, 338, 344, 345, 348, 351, 354, 364,\n",
       "         370, 371, 372, 373, 376, 378, 388, 408, 426, 429, 466, 470, 473]),\n",
       "  'r2_score': 0.9953397420410414,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c4',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c55',\n",
       "   'c60',\n",
       "   'c63',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c74',\n",
       "   'c5_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c19_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c35_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c9_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 104},\n",
       " 41: {'alpha': 0.10224700780014613,\n",
       "  'selected_features': array([  1,   3,   4,   5,   6,   7,   9,  10,  12,  13,  15,  18,  20,\n",
       "          23,  25,  26,  27,  28,  29,  30,  31,  33,  34,  36,  37,  42,\n",
       "          44,  46,  47,  48,  49,  50,  51,  58,  60,  64,  66,  67,  70,\n",
       "          72,  73,  75,  77,  85,  88,  91,  92,  99, 103, 108, 119, 124,\n",
       "         132, 133, 139, 145, 153, 168, 169, 170, 172, 173, 174, 178, 200,\n",
       "         201, 209, 211, 212, 213, 225, 227, 233, 234, 242, 249, 275, 276,\n",
       "         284, 285, 294, 298, 316, 318, 320, 324, 325, 326, 327, 333, 337,\n",
       "         338, 343, 346, 347, 348, 350, 372, 373, 376, 378, 384, 389, 413,\n",
       "         415, 418, 422, 425, 428, 435, 439, 452, 453, 459, 460, 461, 464,\n",
       "         465, 475]),\n",
       "  'r2_score': 0.9895335298488247,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c16',\n",
       "   'c19',\n",
       "   'c21',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c43',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c78',\n",
       "   'c6_lag_1',\n",
       "   'c9_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c9_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c53_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c3_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c7_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c14_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c76_lag_5'],\n",
       "  'n_features': 119},\n",
       " 42: {'alpha': 0.11698393502330097,\n",
       "  'selected_features': array([  1,   2,   3,   8,   9,  10,  11,  12,  13,  19,  20,  21,  24,\n",
       "          25,  27,  29,  32,  34,  37,  38,  40,  43,  46,  47,  48,  50,\n",
       "          51,  52,  57,  60,  61,  62,  63,  65,  66,  67,  68,  69,  70,\n",
       "          72,  76,  78,  93,  99, 101, 108, 111, 118, 119, 123, 135, 137,\n",
       "         138, 141, 142, 144, 151, 152, 171, 175, 176, 179, 185, 191, 211,\n",
       "         219, 224, 225, 229, 237, 238, 241, 243, 244, 245, 246, 252, 253,\n",
       "         259, 264, 269, 270, 271, 277, 279, 280, 285, 289, 290, 295, 296,\n",
       "         326, 328, 329, 332, 336, 337, 338, 339, 347, 348, 373, 394, 402,\n",
       "         414, 428, 435, 442, 443, 460, 463, 465, 471, 472, 474, 476]),\n",
       "  'r2_score': 0.9918019210323975,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c33',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c41',\n",
       "   'c44',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c14_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c32_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c12_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c20_lag_2',\n",
       "   'c26_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c51_lag_3',\n",
       "   'c56_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c7_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 116},\n",
       " 43: {'alpha': 0.09849409269061205,\n",
       "  'selected_features': array([  1,   2,   3,   4,   5,   7,  10,  11,  12,  14,  18,  20,  21,\n",
       "          22,  25,  27,  29,  31,  32,  37,  43,  44,  46,  47,  49,  50,\n",
       "          52,  53,  59,  63,  64,  65,  66,  69,  70,  72,  73,  74,  75,\n",
       "          78,  79,  89,  94, 103, 104, 108, 112, 113, 114, 115, 127, 130,\n",
       "         132, 135, 137, 144, 145, 148, 161, 166, 168, 169, 170, 171, 174,\n",
       "         184, 186, 200, 201, 202, 209, 210, 216, 223, 224, 225, 227, 234,\n",
       "         236, 239, 240, 245, 246, 249, 253, 270, 277, 283, 284, 291, 294,\n",
       "         301, 303, 305, 317, 319, 325, 327, 328, 332, 333, 335, 336, 342,\n",
       "         344, 348, 352, 363, 369, 372, 373, 374, 376, 379, 389, 399, 400,\n",
       "         403, 404, 407, 413, 424, 425, 426, 431, 432, 433, 435, 437, 446,\n",
       "         448, 449, 451, 453, 459, 462, 465, 469, 470, 476]),\n",
       "  'r2_score': 0.9899666398651582,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c15',\n",
       "   'c19',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c38',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c60',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c79',\n",
       "   'c80',\n",
       "   'c10_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c2_lag_2',\n",
       "   'c7_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c27_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c57_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c44_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c6_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c60_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c4_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 140},\n",
       " 44: {'alpha': 0.08920570585848803,\n",
       "  'selected_features': array([  0,   6,   8,   9,  10,  11,  12,  13,  14,  18,  19,  20,  23,\n",
       "          24,  25,  26,  28,  29,  30,  31,  32,  35,  37,  38,  39,  40,\n",
       "          43,  44,  46,  47,  49,  50,  52,  56,  58,  60,  61,  62,  63,\n",
       "          64,  65,  68,  69,  70,  71,  72,  73,  77,  84,  85,  94,  95,\n",
       "          98, 104, 107, 108, 110, 114, 118, 124, 127, 133, 134, 139, 143,\n",
       "         145, 146, 149, 150, 152, 158, 165, 172, 178, 180, 194, 201, 205,\n",
       "         206, 208, 215, 219, 220, 224, 227, 232, 234, 239, 244, 246, 263,\n",
       "         268, 275, 278, 292, 299, 300, 301, 307, 312, 322, 324, 328, 331,\n",
       "         333, 334, 337, 341, 343, 344, 347, 353, 370, 375, 376, 384, 386,\n",
       "         388, 389, 390, 400, 406, 408, 409, 410, 412, 427, 429, 430, 432,\n",
       "         444, 448, 449, 450, 452, 453, 456, 460, 463, 464, 465, 467, 472,\n",
       "         474]),\n",
       "  'r2_score': 0.9790643093921175,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c36',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c57',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c78',\n",
       "   'c5_lag_1',\n",
       "   'c6_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c21_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c61_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c5_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c53_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c73_lag_3',\n",
       "   'c3_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c22_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c71_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c13_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c75_lag_5'],\n",
       "  'n_features': 144},\n",
       " 45: {'alpha': 0.09396629445830121,\n",
       "  'selected_features': array([  2,   6,   7,   8,   9,  11,  12,  13,  21,  22,  23,  24,  25,\n",
       "          27,  28,  30,  31,  32,  33,  35,  36,  38,  39,  41,  43,  44,\n",
       "          45,  46,  47,  48,  49,  50,  51,  52,  62,  63,  65,  66,  68,\n",
       "          69,  70,  79,  89,  99, 107, 108, 109, 113, 114, 119, 123, 131,\n",
       "         132, 133, 135, 137, 143, 144, 146, 148, 151, 163, 164, 172, 175,\n",
       "         177, 178, 185, 195, 200, 201, 204, 217, 219, 223, 226, 228, 230,\n",
       "         231, 232, 233, 235, 236, 238, 244, 246, 274, 290, 291, 292, 304,\n",
       "         305, 307, 308, 313, 315, 323, 328, 331, 334, 335, 337, 338, 343,\n",
       "         347, 348, 360, 361, 365, 373, 374, 375, 376, 378, 398, 399, 403,\n",
       "         409, 414, 418, 424, 425, 426, 429, 430, 431, 432, 434, 439, 450,\n",
       "         451, 454, 461, 472]),\n",
       "  'r2_score': 0.9846547403627262,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c80',\n",
       "   'c10_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c4_lag_2',\n",
       "   'c5_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c26_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c5_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c51_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c53_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c73_lag_5'],\n",
       "  'n_features': 134},\n",
       " 46: {'alpha': 0.12129839315187596,\n",
       "  'selected_features': array([  0,   1,   5,   6,   8,  10,  11,  12,  13,  14,  15,  19,  24,\n",
       "          27,  28,  29,  30,  31,  36,  39,  40,  41,  43,  44,  46,  50,\n",
       "          51,  53,  55,  57,  60,  65,  67,  68,  70,  73,  77,  88,  91,\n",
       "          92,  93,  97,  98, 106, 107, 109, 129, 133, 134, 158, 162, 166,\n",
       "         168, 171, 175, 176, 209, 210, 214, 221, 222, 223, 234, 241, 251,\n",
       "         259, 260, 265, 267, 272, 274, 279, 287, 305, 306, 309, 314, 315,\n",
       "         318, 323, 325, 332, 336, 337, 338, 346, 347, 350, 369, 371, 372,\n",
       "         375, 376, 377, 387, 388, 398, 410, 414, 415, 418, 419, 424, 425,\n",
       "         428, 429, 436, 452, 464, 465, 471, 472, 475]),\n",
       "  'r2_score': 0.9898773893994005,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c16',\n",
       "   'c20',\n",
       "   'c25',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c37',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c56',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c74',\n",
       "   'c78',\n",
       "   'c9_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c3_lag_2',\n",
       "   'c7_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c62_lag_2',\n",
       "   'c63_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c40_lag_3',\n",
       "   'c48_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c67_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c11_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c76_lag_5'],\n",
       "  'n_features': 113},\n",
       " 47: {'alpha': 0.06949270414789283,\n",
       "  'selected_features': array([  0,   1,   3,   4,   5,   6,   9,  10,  12,  14,  17,  18,  19,\n",
       "          20,  21,  23,  24,  25,  26,  29,  31,  32,  33,  34,  37,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  50,  52,  53,  59,  60,  61,\n",
       "          62,  63,  64,  65,  66,  68,  69,  73,  74,  76,  79,  89, 102,\n",
       "         104, 107, 108, 114, 116, 123, 124, 125, 129, 131, 134, 138, 140,\n",
       "         143, 146, 148, 152, 172, 174, 175, 176, 177, 178, 190, 199, 200,\n",
       "         203, 210, 216, 221, 224, 225, 226, 228, 230, 234, 235, 236, 238,\n",
       "         239, 241, 243, 247, 248, 249, 251, 252, 253, 260, 261, 264, 267,\n",
       "         270, 271, 272, 276, 283, 299, 303, 307, 310, 311, 312, 313, 314,\n",
       "         318, 319, 321, 322, 323, 327, 328, 332, 333, 342, 348, 352, 359,\n",
       "         361, 369, 378, 384, 385, 386, 387, 388, 389, 394, 395, 396, 403,\n",
       "         404, 405, 406, 408, 409, 411, 414, 416, 429, 433, 440, 449, 452,\n",
       "         458, 462, 463, 464, 467, 472, 473, 478]),\n",
       "  'r2_score': 0.9831100598504904,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c15',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c77',\n",
       "   'c80',\n",
       "   'c10_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c13_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c57_lag_2',\n",
       "   'c62_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c44_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c73_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c40_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c77_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 164},\n",
       " 48: {'alpha': 0.2463566028008268,\n",
       "  'selected_features': array([  2,   3,   4,  10,  11,  12,  13,  14,  20,  21,  28,  29,  30,\n",
       "          31,  32,  33,  34,  35,  40,  41,  42,  43,  45,  47,  49,  52,\n",
       "          53,  57,  58,  60,  61,  66,  67,  68,  70,  74,  89,  90,  91,\n",
       "          92,  95,  98, 104, 108, 109, 111, 114, 119, 129, 137, 139, 143,\n",
       "         144, 167, 168, 171, 176, 199, 217, 218, 224, 228, 234, 235, 244,\n",
       "         251, 265, 278, 298, 318, 319, 323, 331, 338, 343, 344, 345, 347,\n",
       "         349, 353, 371, 377, 378, 385, 388, 424, 426, 429, 433, 453, 455,\n",
       "         456, 460, 461, 462, 463, 465, 466, 467, 470, 477, 478]),\n",
       "  'r2_score': 0.9582205265700716,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c36',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c75',\n",
       "   'c10_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c32_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c8_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c5_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c25_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c78_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 102},\n",
       " 49: {'alpha': 0.28494127399242003,\n",
       "  'selected_features': array([  3,   5,  10,  11,  12,  13,  17,  18,  20,  23,  24,  28,  30,\n",
       "          31,  32,  33,  34,  36,  37,  40,  45,  47,  50,  52,  62,  63,\n",
       "          64,  66,  67,  70,  72,  73,  77,  93, 101, 102, 113, 129, 132,\n",
       "         133, 159, 160, 161, 199, 201, 228, 229, 231, 232, 237, 251, 253,\n",
       "         272, 273, 278, 323, 324, 337, 341, 345, 352, 365, 366, 398, 403,\n",
       "         404, 406, 408, 424, 425, 426, 458, 461, 463, 464, 465, 466, 467,\n",
       "         468]),\n",
       "  'r2_score': 0.9744456068308505,\n",
       "  'selected_features_names': ['c4',\n",
       "   'c6',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c21',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c41',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c78',\n",
       "   'c14_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c2_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c12_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c22_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5'],\n",
       "  'n_features': 79},\n",
       " 50: {'alpha': 0.09629849271113552,\n",
       "  'selected_features': array([  4,   5,   7,   8,   9,  11,  12,  13,  14,  19,  20,  21,  23,\n",
       "          24,  26,  27,  28,  29,  30,  32,  33,  34,  41,  43,  45,  46,\n",
       "          47,  48,  50,  51,  52,  53,  59,  62,  63,  65,  67,  70,  71,\n",
       "          73,  74,  79,  82,  83,  85,  91,  94, 103, 104, 105, 107, 109,\n",
       "         110, 113, 115, 117, 120, 123, 126, 127, 129, 135, 139, 141, 144,\n",
       "         151, 153, 163, 166, 168, 172, 173, 174, 175, 177, 188, 190, 196,\n",
       "         197, 199, 201, 202, 206, 208, 227, 230, 231, 236, 240, 244, 246,\n",
       "         247, 249, 251, 267, 271, 289, 300, 316, 323, 329, 330, 334, 336,\n",
       "         338, 340, 345, 348, 350, 351, 352, 353, 361, 368, 378, 383, 394,\n",
       "         396, 407, 409, 411, 421, 422, 429, 434, 441, 442, 443, 444, 449,\n",
       "         451, 453, 454, 455, 459, 460, 463, 464, 465, 469]),\n",
       "  'r2_score': 0.9710306154192322,\n",
       "  'selected_features_names': ['c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c60',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c80',\n",
       "   'c3_lag_1',\n",
       "   'c4_lag_1',\n",
       "   'c6_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c4_lag_2',\n",
       "   'c7_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c38_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c21_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c64_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c77_lag_4',\n",
       "   'c8_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c70_lag_5'],\n",
       "  'n_features': 140},\n",
       " 51: {'alpha': 0.05828989864097448,\n",
       "  'selected_features': array([  1,   4,   5,   7,   8,   9,  10,  11,  12,  14,  17,  19,  20,\n",
       "          24,  25,  27,  28,  29,  30,  31,  33,  34,  37,  42,  43,  44,\n",
       "          45,  46,  47,  48,  49,  50,  52,  53,  56,  57,  58,  59,  62,\n",
       "          63,  64,  65,  66,  67,  68,  69,  73,  75,  76,  77,  85,  93,\n",
       "          96,  98, 100, 104, 105, 106, 109, 111, 114, 116, 120, 123, 128,\n",
       "         129, 130, 131, 133, 137, 139, 141, 143, 144, 146, 148, 153, 157,\n",
       "         161, 164, 165, 168, 173, 174, 177, 180, 186, 187, 189, 194, 195,\n",
       "         198, 203, 204, 205, 207, 210, 211, 213, 216, 222, 223, 225, 226,\n",
       "         228, 230, 231, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243,\n",
       "         247, 248, 250, 253, 260, 261, 264, 265, 266, 268, 271, 289, 302,\n",
       "         303, 308, 314, 318, 319, 326, 327, 330, 333, 334, 335, 336, 337,\n",
       "         338, 342, 344, 345, 364, 365, 368, 369, 372, 374, 375, 376, 377,\n",
       "         384, 385, 386, 406, 407, 408, 409, 414, 420, 421, 424, 425, 429,\n",
       "         438, 444, 446, 449, 451, 452, 455, 467, 468, 469, 473, 475, 478]),\n",
       "  'r2_score': 0.9314060643129533,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c15',\n",
       "   'c18',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c6_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c32_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c78_lag_1',\n",
       "   'c2_lag_2',\n",
       "   'c5_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c21_lag_2',\n",
       "   'c27_lag_2',\n",
       "   'c28_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c57_lag_2',\n",
       "   'c63_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c7_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c7_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c76_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 182},\n",
       " 52: {'alpha': 0.14713827929980525,\n",
       "  'selected_features': array([  1,   5,   6,   7,   8,  10,  12,  13,  23,  24,  26,  27,  29,\n",
       "          30,  31,  33,  34,  37,  41,  42,  43,  45,  47,  50,  51,  52,\n",
       "          53,  60,  63,  64,  68,  69,  70,  71,  75,  76,  78,  87,  88,\n",
       "          89,  90,  98, 103, 113, 123, 124, 126, 127, 131, 134, 135, 137,\n",
       "         138, 141, 142, 144, 154, 168, 173, 177, 190, 193, 208, 220, 222,\n",
       "         224, 225, 233, 234, 239, 247, 249, 251, 266, 267, 268, 274, 294,\n",
       "         329, 333, 336, 337, 338, 339, 340, 344, 348, 349, 350, 351, 352,\n",
       "         370, 372, 373, 378, 408, 410, 424, 430, 434, 435, 436, 438, 441,\n",
       "         442, 444, 446, 450, 451, 452, 455, 459, 460, 462, 472, 473]),\n",
       "  'r2_score': 0.9845684403400654,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c61',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c8_lag_1',\n",
       "   'c9_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c75_lag_1',\n",
       "   'c9_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c61_lag_2',\n",
       "   'c63_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c8_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c10_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c21_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c9_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 116},\n",
       " 53: {'alpha': 0.1331953044185073,\n",
       "  'selected_features': array([  0,   1,   2,   4,   5,  11,  12,  18,  23,  24,  31,  32,  33,\n",
       "          34,  35,  42,  43,  44,  47,  48,  49,  50,  52,  53,  56,  57,\n",
       "          58,  60,  62,  63,  64,  65,  66,  67,  68,  69,  70,  72,  73,\n",
       "          85,  92,  94,  99, 113, 115, 119, 129, 130, 135, 139, 141, 147,\n",
       "         175, 176, 185, 193, 198, 199, 204, 206, 207, 209, 222, 224, 225,\n",
       "         226, 228, 230, 234, 238, 239, 240, 241, 242, 243, 244, 246, 249,\n",
       "         252, 253, 266, 268, 284, 288, 291, 296, 298, 305, 309, 320, 322,\n",
       "         324, 342, 344, 347, 351, 360, 367, 368, 369, 371, 373, 375, 378,\n",
       "         384, 385, 386, 387, 398, 399, 404, 408, 412, 417, 418, 422, 426,\n",
       "         428, 431, 443, 444, 446, 452, 456, 459, 461, 463, 466, 472, 476,\n",
       "         477]),\n",
       "  'r2_score': 0.9258417204396361,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c19',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c36',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c6_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c26_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c63_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c7_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c48_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c5_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c13_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c77_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 131},\n",
       " 54: {'alpha': 0.08632939734471363,\n",
       "  'selected_features': array([  3,   4,   5,   6,   7,   8,   9,  10,  12,  13,  15,  20,  23,\n",
       "          26,  28,  29,  30,  32,  37,  39,  43,  46,  47,  48,  49,  50,\n",
       "          51,  52,  53,  58,  60,  62,  63,  66,  68,  69,  70,  71,  72,\n",
       "          73,  74,  79,  85,  86,  87,  88,  93,  96,  97,  98, 104, 109,\n",
       "         110, 112, 113, 116, 117, 122, 123, 126, 127, 129, 130, 132, 134,\n",
       "         137, 138, 145, 149, 154, 158, 159, 168, 172, 173, 178, 199, 200,\n",
       "         210, 218, 219, 221, 224, 225, 230, 232, 234, 236, 239, 240, 243,\n",
       "         245, 247, 249, 252, 256, 261, 265, 268, 270, 271, 272, 274, 275,\n",
       "         278, 284, 290, 301, 303, 304, 317, 320, 323, 327, 332, 336, 338,\n",
       "         343, 347, 349, 350, 353, 367, 370, 374, 375, 376, 399, 404, 408,\n",
       "         416, 418, 439, 446, 447, 454, 475]),\n",
       "  'r2_score': 0.9833176186495955,\n",
       "  'selected_features_names': ['c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c16',\n",
       "   'c21',\n",
       "   'c24',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c38',\n",
       "   'c40',\n",
       "   'c44',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c80',\n",
       "   'c6_lag_1',\n",
       "   'c7_lag_1',\n",
       "   'c8_lag_1',\n",
       "   'c9_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c75_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c9_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c62_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c17_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c51_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c48_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c5_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c76_lag_5'],\n",
       "  'n_features': 137},\n",
       " 55: {'alpha': 0.004154704005903566,\n",
       "  'selected_features': array([ 11,  12,  15,  16,  18,  19,  25,  31,  32,  33,  35,  36,  55,\n",
       "          57,  60,  74,  75,  78, 114, 116, 118, 126, 138, 153, 171, 172,\n",
       "         173, 175, 176, 213, 215, 233, 237, 238, 244, 247, 248, 251, 263,\n",
       "         264, 265, 297, 298, 299, 315, 333, 346, 347, 355, 369, 393, 434,\n",
       "         435, 449, 453, 458, 460, 466, 474, 478]),\n",
       "  'r2_score': 0.9980944902642694,\n",
       "  'selected_features_names': ['c12',\n",
       "   'c13',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c26',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c56',\n",
       "   'c58',\n",
       "   'c61',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c79',\n",
       "   'c35_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c5_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c14_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c36_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c35_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 60},\n",
       " 56: {'alpha': 0.0030588365434254805,\n",
       "  'selected_features': array([ 13,  15,  17,  18,  19,  31,  32,  33,  34,  37,  46,  55,  56,\n",
       "          58,  61,  63,  65,  74,  75,  76,  77,  78, 108, 109, 113, 118,\n",
       "         126, 129, 132, 133, 138, 140, 146, 151, 155, 162, 169, 170, 176,\n",
       "         179, 180, 181, 194, 206, 207, 208, 211, 213, 219, 225, 232, 233,\n",
       "         235, 247, 248, 250, 251, 262, 265, 266, 270, 271, 273, 277, 278,\n",
       "         280, 284, 292, 315, 317, 333, 334, 336, 339, 343, 351, 359, 361,\n",
       "         364, 376, 402, 411, 417, 418, 424, 434, 439, 444, 453, 459, 460,\n",
       "         462, 466]),\n",
       "  'r2_score': 0.9930394958416057,\n",
       "  'selected_features_names': ['c14',\n",
       "   'c16',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c47',\n",
       "   'c56',\n",
       "   'c57',\n",
       "   'c59',\n",
       "   'c62',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c29_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c76_lag_1',\n",
       "   'c3_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c20_lag_2',\n",
       "   'c21_lag_2',\n",
       "   'c22_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c53_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c40_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c67_lag_5'],\n",
       "  'n_features': 93},\n",
       " 57: {'alpha': 0.0031455785901035576,\n",
       "  'selected_features': array([  6,  10,  12,  13,  16,  17,  19,  21,  31,  34,  36,  39,  45,\n",
       "          47,  48,  49,  51,  53,  55,  56,  57,  58,  63,  74,  78,  94,\n",
       "         110, 125, 138, 145, 146, 158, 163, 165, 167, 178, 184, 187, 188,\n",
       "         199, 203, 212, 229, 230, 233, 234, 237, 238, 240, 243, 260, 261,\n",
       "         264, 265, 266, 271, 287, 299, 316, 321, 322, 323, 329, 334, 335,\n",
       "         336, 343, 348, 354, 364, 374, 383, 393, 402, 406, 413, 414, 436,\n",
       "         444, 470, 471, 472, 473, 476, 477]),\n",
       "  'r2_score': 0.9967005933027887,\n",
       "  'selected_features_names': ['c7',\n",
       "   'c11',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c20',\n",
       "   'c22',\n",
       "   'c32',\n",
       "   'c35',\n",
       "   'c37',\n",
       "   'c40',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c56',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c64',\n",
       "   'c75',\n",
       "   'c79',\n",
       "   'c15_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c4_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c28_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c53_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c48_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c35_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c64_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c77_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 85},\n",
       " 58: {'alpha': 0.003324612976500782,\n",
       "  'selected_features': array([  6,  15,  16,  28,  29,  36,  37,  38,  39,  40,  43,  57,  58,\n",
       "          62,  66,  69,  76,  77,  78,  87,  94,  95, 101, 102, 107, 120,\n",
       "         125, 150, 153, 170, 171, 172, 178, 198, 199, 200, 201, 214, 233,\n",
       "         240, 254, 260, 302, 309, 322, 323, 324, 338, 348, 374, 387, 389,\n",
       "         400, 402, 405, 409, 425, 429, 444, 452, 457, 460, 463, 467, 470,\n",
       "         476]),\n",
       "  'r2_score': 0.9962414799404212,\n",
       "  'selected_features_names': ['c7',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c44',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c63',\n",
       "   'c67',\n",
       "   'c70',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c8_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c15_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 66},\n",
       " 59: {'alpha': 0.004358183160208934,\n",
       "  'selected_features': array([  1,  12,  14,  15,  16,  17,  35,  36,  39,  41,  46,  51,  56,\n",
       "          57,  58,  61,  72,  74,  76,  78,  87,  98, 102, 103, 106, 112,\n",
       "         114, 130, 153, 170, 171, 172, 175, 178, 199, 224, 226, 230, 247,\n",
       "         254, 268, 269, 328, 329, 337, 339, 345, 348, 359, 362, 368, 378,\n",
       "         386, 387, 388, 404, 419, 441, 448, 449, 462, 469, 472, 475]),\n",
       "  'r2_score': 0.9958585297563516,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c13',\n",
       "   'c15',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c40',\n",
       "   'c42',\n",
       "   'c47',\n",
       "   'c52',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c62',\n",
       "   'c73',\n",
       "   'c75',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c8_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c74_lag_1',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c8_lag_3',\n",
       "   'c15_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c9_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c40_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c5_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c76_lag_5'],\n",
       "  'n_features': 64},\n",
       " 60: {'alpha': 0.1559661868620428,\n",
       "  'selected_features': array([  0,   1,   3,   9,  10,  11,  12,  20,  21,  22,  28,  30,  33,\n",
       "          36,  38,  40,  42,  43,  44,  50,  52,  53,  60,  64,  65,  67,\n",
       "          68,  69,  70,  71,  72,  74,  76,  77,  78,  89,  91,  99, 101,\n",
       "         103, 105, 126, 145, 146, 174, 175, 176, 177, 178, 193, 211, 226,\n",
       "         235, 239, 240, 241, 250, 251, 252, 257, 264, 271, 275, 278, 286,\n",
       "         288, 308, 323, 326, 332, 333, 343, 344, 353, 369, 374, 379, 386,\n",
       "         388, 404, 419, 420, 421, 422, 423, 426, 429, 432, 437, 444, 446,\n",
       "         448, 462, 464, 465, 466, 467, 469]),\n",
       "  'r2_score': 0.9965535846081176,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c4',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c34',\n",
       "   'c37',\n",
       "   'c39',\n",
       "   'c41',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c61',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c75',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c10_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c52_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c2_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c18_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c7_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c60_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c5_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c70_lag_5'],\n",
       "  'n_features': 98},\n",
       " 61: {'alpha': 0.3258345004369018,\n",
       "  'selected_features': array([  2,   6,   7,   8,  12,  13,  14,  21,  22,  24,  25,  29,  31,\n",
       "          32,  36,  41,  43,  44,  47,  50,  51,  52,  53,  60,  61,  62,\n",
       "          63,  64,  67,  68,  70,  72,  73,  75,  76,  79,  89,  94, 110,\n",
       "         130, 137, 138, 139, 145, 146, 157, 167, 168, 174, 184, 188, 199,\n",
       "         227, 228, 240, 242, 247, 248, 250, 251, 256, 278, 282, 297, 320,\n",
       "         321, 324, 325, 327, 328, 334, 347, 362, 375, 382, 384, 395, 398,\n",
       "         417, 434, 449, 457, 464, 466, 468, 474]),\n",
       "  'r2_score': 0.9818684732731926,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c37',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c48',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c80',\n",
       "   'c10_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c78_lag_1',\n",
       "   'c8_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c17_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c43_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c2_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c63_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c18_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c75_lag_5'],\n",
       "  'n_features': 86},\n",
       " 62: {'alpha': 0.10462407312411307,\n",
       "  'selected_features': array([  1,   2,   3,   5,   6,  10,  11,  23,  24,  25,  26,  30,  32,\n",
       "          33,  38,  42,  44,  45,  47,  48,  51,  52,  53,  59,  61,  64,\n",
       "          65,  66,  68,  70,  73,  75,  89,  90,  94, 101, 104, 108, 110,\n",
       "         125, 129, 130, 134, 136, 139, 140, 148, 151, 166, 172, 192, 193,\n",
       "         194, 198, 201, 205, 208, 209, 212, 218, 223, 231, 232, 234, 237,\n",
       "         238, 239, 251, 253, 255, 256, 263, 267, 272, 273, 289, 298, 301,\n",
       "         302, 303, 304, 305, 317, 318, 322, 330, 335, 339, 348, 358, 365,\n",
       "         369, 370, 371, 387, 388, 389, 401, 421, 427, 434, 440, 442, 446,\n",
       "         458, 459, 460, 464, 467, 470, 471, 473, 474]),\n",
       "  'r2_score': 0.9928286546513724,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c31',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c39',\n",
       "   'c43',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c60',\n",
       "   'c62',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c10_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c7_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c33_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c53_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c12_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c16_lag_3',\n",
       "   'c17_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c3_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c39_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c75_lag_5'],\n",
       "  'n_features': 113},\n",
       " 63: {'alpha': 0.09441696689864147,\n",
       "  'selected_features': array([  0,   1,   2,   3,   4,   5,   7,  10,  11,  12,  13,  17,  18,\n",
       "          20,  24,  26,  27,  28,  29,  30,  31,  32,  36,  37,  42,  44,\n",
       "          45,  47,  49,  50,  51,  53,  54,  60,  61,  63,  66,  70,  71,\n",
       "          72,  74,  78,  87,  99, 104, 111, 122, 124, 129, 133, 138, 140,\n",
       "         141, 142, 144, 149, 150, 167, 199, 208, 224, 225, 233, 234, 235,\n",
       "         238, 239, 244, 253, 264, 265, 276, 281, 282, 294, 296, 297, 301,\n",
       "         311, 321, 322, 323, 329, 330, 331, 333, 334, 346, 347, 348, 352,\n",
       "         353, 359, 360, 361, 368, 370, 371, 372, 373, 378, 384, 394, 395,\n",
       "         398, 412, 413, 414, 430, 437, 438, 447, 448, 451, 452, 453, 459,\n",
       "         460, 465, 467, 471, 472, 473]),\n",
       "  'r2_score': 0.9935363920390289,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c21',\n",
       "   'c25',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c43',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c55',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c64',\n",
       "   'c67',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c75',\n",
       "   'c79',\n",
       "   'c8_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c32_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c8_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c5_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c42_lag_3',\n",
       "   'c43_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c40_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c13_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 123},\n",
       " 64: {'alpha': 0.08701805872117101,\n",
       "  'selected_features': array([  0,   2,   3,   5,   6,  10,  11,  12,  14,  16,  18,  19,  21,\n",
       "          22,  24,  26,  28,  30,  31,  32,  36,  41,  43,  44,  45,  47,\n",
       "          48,  49,  50,  51,  52,  53,  54,  58,  59,  61,  62,  64,  65,\n",
       "          66,  67,  68,  71,  72,  73,  76,  79,  89,  90,  91,  92,  96,\n",
       "          97, 107, 108, 112, 114, 119, 123, 126, 131, 132, 138, 139, 151,\n",
       "         156, 159, 178, 183, 184, 194, 196, 204, 208, 210, 216, 224, 228,\n",
       "         232, 233, 235, 238, 239, 241, 243, 253, 263, 270, 271, 272, 291,\n",
       "         292, 294, 296, 297, 298, 299, 316, 318, 321, 328, 330, 331, 332,\n",
       "         333, 336, 337, 338, 344, 350, 355, 363, 366, 369, 374, 378, 382,\n",
       "         399, 408, 410, 414, 419, 424, 438, 443, 452, 453, 454, 455, 461,\n",
       "         462, 464, 468, 472, 474, 478]),\n",
       "  'r2_score': 0.9882415731384641,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c15',\n",
       "   'c17',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c25',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c37',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c55',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c77',\n",
       "   'c80',\n",
       "   'c10_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c12_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c77_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c19_lag_2',\n",
       "   'c24_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c57_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c53_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c36_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c63_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c9_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c56_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 136},\n",
       " 65: {'alpha': 0.13928914123854272,\n",
       "  'selected_features': array([  0,   3,   4,   6,   7,   9,  10,  11,  12,  13,  20,  22,  23,\n",
       "          26,  28,  30,  31,  32,  33,  34,  37,  38,  39,  41,  43,  45,\n",
       "          47,  49,  50,  51,  53,  57,  59,  61,  62,  64,  66,  67,  70,\n",
       "          71,  72,  75,  76,  79,  85, 102, 113, 115, 119, 120, 128, 130,\n",
       "         134, 138, 144, 145, 146, 147, 171, 173, 174, 178, 180, 189, 190,\n",
       "         191, 202, 206, 207, 219, 221, 228, 233, 234, 235, 236, 240, 241,\n",
       "         242, 245, 249, 250, 253, 271, 284, 291, 297, 302, 309, 310, 324,\n",
       "         325, 327, 329, 331, 333, 334, 338, 349, 352, 353, 360, 361, 362,\n",
       "         370, 371, 372, 384, 386, 387, 403, 404, 407, 409, 414, 417, 418,\n",
       "         424, 432, 448, 458, 459, 463, 464, 465, 466]),\n",
       "  'r2_score': 0.9780421845399585,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c21',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c48',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c58',\n",
       "   'c60',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c65',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c80',\n",
       "   'c6_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c34_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c12_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c21_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c43_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c62_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c45_lag_3',\n",
       "   'c52_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c5_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c8_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c59_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5'],\n",
       "  'n_features': 126},\n",
       " 66: {'alpha': 0.08484574690574824,\n",
       "  'selected_features': array([  1,   2,   3,   6,   8,   9,  10,  11,  12,  13,  14,  24,  25,\n",
       "          27,  29,  30,  31,  32,  33,  34,  36,  41,  42,  43,  44,  45,\n",
       "          46,  47,  48,  49,  50,  51,  52,  53,  58,  62,  64,  65,  66,\n",
       "          67,  68,  69,  70,  72,  83,  85,  89,  94, 107, 117, 122, 124,\n",
       "         130, 136, 139, 140, 144, 149, 151, 165, 171, 175, 176, 177, 178,\n",
       "         179, 180, 187, 188, 189, 192, 194, 195, 197, 199, 201, 204, 205,\n",
       "         209, 219, 223, 224, 226, 227, 228, 230, 231, 232, 233, 234, 237,\n",
       "         238, 239, 240, 242, 243, 247, 258, 262, 263, 270, 273, 283, 289,\n",
       "         294, 296, 304, 307, 309, 316, 318, 320, 323, 329, 332, 334, 336,\n",
       "         338, 345, 347, 348, 352, 356, 359, 364, 367, 370, 384, 389, 393,\n",
       "         395, 397, 399, 401, 408, 409, 419, 420, 423, 429, 430, 431, 433,\n",
       "         445, 452, 453, 461, 473, 477]),\n",
       "  'r2_score': 0.9737876099658134,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c7',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c37',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c59',\n",
       "   'c63',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c4_lag_1',\n",
       "   'c6_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c38_lag_1',\n",
       "   'c43_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c20_lag_2',\n",
       "   'c21_lag_2',\n",
       "   'c28_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c33_lag_2',\n",
       "   'c35_lag_2',\n",
       "   'c36_lag_2',\n",
       "   'c38_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c60_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c19_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c34_lag_3',\n",
       "   'c44_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c37_lag_4',\n",
       "   'c40_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c48_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c78_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 149},\n",
       " 67: {'alpha': 0.11165393194699674,\n",
       "  'selected_features': array([  1,   3,   4,   8,   9,  10,  11,  13,  15,  24,  26,  27,  28,\n",
       "          29,  30,  31,  32,  39,  40,  41,  43,  45,  48,  49,  51,  53,\n",
       "          61,  65,  67,  68,  70,  72,  73,  76,  79,  80,  90,  93,  94,\n",
       "         103, 110, 119, 120, 121, 123, 124, 126, 129, 131, 136, 137, 138,\n",
       "         139, 144, 147, 149, 154, 157, 165, 167, 170, 171, 172, 175, 179,\n",
       "         198, 203, 208, 210, 218, 223, 226, 232, 239, 251, 253, 259, 264,\n",
       "         268, 278, 281, 282, 295, 300, 303, 314, 316, 325, 326, 329, 331,\n",
       "         336, 337, 339, 346, 348, 350, 352, 353, 366, 369, 371, 374, 376,\n",
       "         378, 391, 393, 394, 396, 399, 408, 414, 416, 423, 425, 427, 450,\n",
       "         451, 452, 457, 468, 470]),\n",
       "  'r2_score': 0.9897007757587232,\n",
       "  'selected_features_names': ['c2',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c14',\n",
       "   'c16',\n",
       "   'c25',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c62',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c77',\n",
       "   'c80',\n",
       "   'c1_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c75_lag_1',\n",
       "   'c78_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c20_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c12_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c42_lag_3',\n",
       "   'c43_lag_3',\n",
       "   'c56_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c6_lag_4',\n",
       "   'c7_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c72_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c77_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c9_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c52_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c71_lag_5'],\n",
       "  'n_features': 122},\n",
       " 68: {'alpha': 0.09574123230699405,\n",
       "  'selected_features': array([  2,   6,   7,   9,  10,  11,  12,  17,  19,  25,  26,  27,  28,\n",
       "          29,  30,  31,  32,  33,  41,  46,  47,  48,  49,  50,  51,  53,\n",
       "          56,  60,  61,  63,  64,  65,  66,  71,  72,  73,  76,  78,  93,\n",
       "          97,  99, 103, 104, 109, 110, 112, 124, 126, 132, 133, 136, 140,\n",
       "         149, 150, 151, 152, 165, 167, 170, 171, 172, 173, 175, 176, 177,\n",
       "         183, 184, 200, 201, 204, 213, 214, 215, 218, 227, 228, 229, 230,\n",
       "         231, 232, 233, 235, 236, 240, 250, 251, 252, 263, 269, 288, 299,\n",
       "         300, 307, 308, 317, 320, 321, 323, 324, 335, 338, 344, 348, 361,\n",
       "         362, 394, 395, 398, 401, 402, 405, 414, 419, 427, 434, 436, 446,\n",
       "         454, 464, 465, 466, 469, 471, 472]),\n",
       "  'r2_score': 0.9817226261826872,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c18',\n",
       "   'c20',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c42',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c57',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c64',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c72',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c14_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c24_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c49_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c2_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c43_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c79_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c6_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c73_lag_5'],\n",
       "  'n_features': 124},\n",
       " 69: {'alpha': 0.06950758334264402,\n",
       "  'selected_features': array([  0,   1,   2,   3,   6,   7,   9,  10,  11,  12,  13,  17,  18,\n",
       "          19,  20,  24,  25,  26,  27,  31,  32,  33,  34,  37,  40,  42,\n",
       "          44,  45,  46,  47,  48,  49,  51,  52,  53,  54,  58,  59,  61,\n",
       "          62,  63,  66,  67,  69,  70,  73,  75,  76,  77,  84,  93, 104,\n",
       "         105, 106, 107, 110, 124, 125, 126, 129, 130, 139, 142, 146, 148,\n",
       "         149, 158, 164, 173, 178, 186, 188, 189, 200, 201, 204, 206, 209,\n",
       "         210, 215, 217, 224, 225, 227, 228, 230, 235, 237, 238, 242, 244,\n",
       "         245, 248, 251, 256, 261, 263, 266, 267, 272, 275, 282, 289, 306,\n",
       "         307, 311, 314, 317, 319, 323, 326, 329, 332, 334, 339, 347, 351,\n",
       "         353, 366, 367, 370, 374, 375, 378, 385, 393, 399, 403, 406, 409,\n",
       "         413, 424, 428, 433, 434, 436, 438, 442, 444, 445, 450, 453, 460,\n",
       "         461, 464, 465, 470, 474, 475, 476, 477]),\n",
       "  'r2_score': 0.9656924904700204,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c4',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c41',\n",
       "   'c43',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c55',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c64',\n",
       "   'c67',\n",
       "   'c68',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c74',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c5_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c31_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c5_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c27_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c30_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c47_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c3_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c17_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c24_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c43_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c67_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c7_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c48_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c4_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c76_lag_5',\n",
       "   'c77_lag_5',\n",
       "   'c78_lag_5'],\n",
       "  'n_features': 151},\n",
       " 70: {'alpha': 0.07668758197545826,\n",
       "  'selected_features': array([  0,   1,   3,   4,   5,   8,   9,  10,  11,  12,  13,  14,  16,\n",
       "          17,  19,  20,  21,  24,  25,  26,  28,  29,  30,  31,  32,  33,\n",
       "          38,  40,  42,  43,  44,  45,  46,  47,  48,  51,  53,  60,  65,\n",
       "          66,  69,  72,  73,  76,  77,  79,  82,  83,  86,  87,  95,  99,\n",
       "         100, 101, 104, 105, 106, 119, 120, 124, 128, 129, 130, 132, 133,\n",
       "         134, 139, 146, 149, 150, 152, 162, 171, 172, 176, 177, 178, 186,\n",
       "         201, 204, 209, 215, 224, 226, 227, 228, 235, 239, 246, 249, 252,\n",
       "         269, 276, 286, 295, 297, 298, 299, 303, 310, 311, 313, 314, 318,\n",
       "         319, 320, 321, 324, 325, 331, 332, 334, 336, 338, 342, 343, 351,\n",
       "         363, 369, 371, 372, 374, 375, 384, 388, 408, 416, 418, 419, 424,\n",
       "         425, 427, 428, 429, 430, 431, 435, 437, 447, 448, 449, 452, 453,\n",
       "         459, 460, 461, 470, 471, 473]),\n",
       "  'r2_score': 0.945688607601306,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c4',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c20',\n",
       "   'c21',\n",
       "   'c22',\n",
       "   'c25',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c39',\n",
       "   'c41',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c46',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c61',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c70',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c80',\n",
       "   'c3_lag_1',\n",
       "   'c4_lag_1',\n",
       "   'c7_lag_1',\n",
       "   'c8_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c73_lag_1',\n",
       "   'c3_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c27_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c80_lag_2',\n",
       "   'c7_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c47_lag_3',\n",
       "   'c56_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c59_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c2_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c6_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c32_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c9_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c19_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c36_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 149},\n",
       " 71: {'alpha': 0.16138714912640273,\n",
       "  'selected_features': array([  0,   5,   7,   9,  10,  11,  13,  17,  23,  28,  29,  34,  39,\n",
       "          40,  42,  48,  49,  50,  51,  53,  54,  61,  62,  64,  65,  67,\n",
       "          68,  69,  75,  94,  95,  96,  97,  98, 101, 108, 114, 116, 125,\n",
       "         143, 144, 158, 159, 165, 175, 177, 178, 182, 183, 192, 193, 209,\n",
       "         218, 223, 229, 233, 238, 241, 242, 245, 249, 253, 267, 268, 271,\n",
       "         285, 296, 297, 302, 303, 304, 306, 307, 308, 313, 316, 321, 328,\n",
       "         331, 332, 333, 334, 337, 338, 339, 346, 352, 358, 364, 365, 372,\n",
       "         376, 377, 389, 390, 392, 414, 419, 420, 421, 422, 424, 425, 434,\n",
       "         438, 446, 448, 449, 456, 459, 470, 476, 478]),\n",
       "  'r2_score': 0.9833934273023072,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c14',\n",
       "   'c18',\n",
       "   'c24',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c35',\n",
       "   'c40',\n",
       "   'c41',\n",
       "   'c43',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c54',\n",
       "   'c55',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c68',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c76',\n",
       "   'c15_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c18_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c29_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c37_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c23_lag_2',\n",
       "   'c24_lag_2',\n",
       "   'c33_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c67_lag_3',\n",
       "   'c68_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c77_lag_3',\n",
       "   'c2_lag_4',\n",
       "   'c9_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c39_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c57_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c71_lag_4',\n",
       "   'c73_lag_4',\n",
       "   'c15_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c26_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c71_lag_5',\n",
       "   'c77_lag_5',\n",
       "   'c79_lag_5'],\n",
       "  'n_features': 113},\n",
       " 72: {'alpha': 0.07379281998359978,\n",
       "  'selected_features': array([  0,   4,   5,   6,   7,   9,  10,  11,  12,  13,  14,  23,  25,\n",
       "          26,  27,  29,  30,  31,  33,  34,  37,  43,  44,  46,  47,  50,\n",
       "          51,  52,  53,  54,  56,  59,  60,  63,  65,  68,  72,  75,  76,\n",
       "          78,  89,  90, 106, 124, 126, 127, 129, 135, 144, 145, 149, 158,\n",
       "         178, 179, 185, 188, 190, 191, 198, 199, 205, 207, 213, 227, 228,\n",
       "         229, 230, 231, 234, 235, 236, 240, 244, 248, 250, 256, 267, 268,\n",
       "         271, 272, 274, 277, 302, 303, 304, 308, 315, 318, 322, 323, 326,\n",
       "         333, 335, 338, 343, 347, 348, 349, 352, 365, 370, 371, 372, 373,\n",
       "         377, 378, 389, 391, 401, 402, 407, 410, 421, 422, 426, 428, 439,\n",
       "         440, 443, 447, 448, 449, 453, 459, 460, 464, 473, 474]),\n",
       "  'r2_score': 0.9880980964272662,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c7',\n",
       "   'c8',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c38',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c55',\n",
       "   'c57',\n",
       "   'c60',\n",
       "   'c61',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c69',\n",
       "   'c73',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c10_lag_1',\n",
       "   'c11_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c47_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c19_lag_2',\n",
       "   'c20_lag_2',\n",
       "   'c26_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c31_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c48_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c9_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c17_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c35_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c64_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c3_lag_4',\n",
       "   'c4_lag_4',\n",
       "   'c7_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c29_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c59_lag_4',\n",
       "   'c70_lag_4',\n",
       "   'c72_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c22_lag_5',\n",
       "   'c23_lag_5',\n",
       "   'c27_lag_5',\n",
       "   'c29_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c48_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c74_lag_5',\n",
       "   'c75_lag_5'],\n",
       "  'n_features': 128},\n",
       " 73: {'alpha': 0.06776977425755625,\n",
       "  'selected_features': array([  0,   1,   2,   4,   5,   8,   9,  10,  11,  12,  13,  14,  21,\n",
       "          22,  23,  24,  26,  27,  29,  30,  31,  32,  33,  34,  38,  41,\n",
       "          42,  43,  45,  49,  52,  54,  58,  61,  65,  69,  70,  72,  73,\n",
       "          74,  75,  84,  98, 112, 114, 119, 120, 121, 123, 124, 125, 127,\n",
       "         129, 134, 136, 137, 138, 139, 140, 141, 144, 145, 146, 149, 151,\n",
       "         156, 169, 174, 175, 176, 183, 188, 191, 203, 204, 209, 214, 223,\n",
       "         225, 229, 231, 232, 233, 234, 235, 236, 238, 240, 243, 245, 247,\n",
       "         249, 253, 264, 265, 267, 268, 270, 277, 285, 290, 297, 299, 301,\n",
       "         302, 309, 311, 317, 323, 330, 334, 336, 344, 345, 346, 352, 372,\n",
       "         374, 375, 377, 385, 387, 390, 409, 420, 430, 431, 443, 444, 445,\n",
       "         448, 452, 454, 459, 460, 461, 462, 463, 464, 465, 467, 468, 471]),\n",
       "  'r2_score': 0.9679808863626589,\n",
       "  'selected_features_names': ['c1',\n",
       "   'c2',\n",
       "   'c3',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c22',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c27',\n",
       "   'c28',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c39',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c46',\n",
       "   'c50',\n",
       "   'c53',\n",
       "   'c55',\n",
       "   'c59',\n",
       "   'c62',\n",
       "   'c66',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c73',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c76',\n",
       "   'c5_lag_1',\n",
       "   'c19_lag_1',\n",
       "   'c33_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c41_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c58_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c72_lag_1',\n",
       "   'c77_lag_1',\n",
       "   'c10_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c24_lag_2',\n",
       "   'c29_lag_2',\n",
       "   'c32_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c55_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c73_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c75_lag_2',\n",
       "   'c76_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c79_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c8_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c25_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c46_lag_3',\n",
       "   'c51_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c63_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c78_lag_3',\n",
       "   'c4_lag_4',\n",
       "   'c11_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c25_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c53_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c56_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c66_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c71_lag_4',\n",
       "   'c10_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c72_lag_5'],\n",
       "  'n_features': 143},\n",
       " 74: {'alpha': 0.04992332113210493,\n",
       "  'selected_features': array([  2,   4,   5,   7,   8,   9,  10,  11,  12,  13,  14,  17,  19,\n",
       "          22,  23,  25,  26,  28,  29,  30,  31,  32,  33,  34,  36,  37,\n",
       "          40,  41,  42,  43,  44,  46,  47,  48,  49,  50,  51,  52,  53,\n",
       "          54,  58,  60,  61,  62,  64,  65,  66,  68,  69,  70,  71,  73,\n",
       "          74,  76,  78,  86,  89,  92,  94,  95,  96, 104, 105, 106, 107,\n",
       "         114, 119, 123, 124, 127, 130, 131, 133, 134, 135, 136, 138, 139,\n",
       "         141, 142, 143, 144, 145, 146, 148, 150, 162, 163, 169, 173, 174,\n",
       "         175, 176, 177, 184, 198, 203, 204, 209, 210, 213, 215, 217, 221,\n",
       "         222, 223, 226, 227, 229, 230, 233, 236, 241, 242, 244, 245, 249,\n",
       "         251, 253, 259, 260, 261, 262, 265, 266, 268, 270, 272, 277, 278,\n",
       "         281, 292, 294, 295, 296, 299, 300, 301, 304, 305, 309, 314, 318,\n",
       "         319, 322, 324, 329, 331, 332, 334, 336, 337, 338, 342, 349, 350,\n",
       "         352, 356, 366, 369, 371, 373, 377, 386, 387, 394, 399, 400, 404,\n",
       "         406, 407, 408, 415, 416, 419, 420, 423, 424, 429, 432, 437, 438,\n",
       "         442, 444, 445, 446, 448, 449, 460, 461, 462, 463, 464, 465, 467,\n",
       "         468, 469, 470]),\n",
       "  'r2_score': 0.9571943854866743,\n",
       "  'selected_features_names': ['c3',\n",
       "   'c5',\n",
       "   'c6',\n",
       "   'c8',\n",
       "   'c9',\n",
       "   'c10',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c13',\n",
       "   'c14',\n",
       "   'c15',\n",
       "   'c18',\n",
       "   'c20',\n",
       "   'c23',\n",
       "   'c24',\n",
       "   'c26',\n",
       "   'c27',\n",
       "   'c29',\n",
       "   'c30',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c34',\n",
       "   'c35',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c41',\n",
       "   'c42',\n",
       "   'c43',\n",
       "   'c44',\n",
       "   'c45',\n",
       "   'c47',\n",
       "   'c48',\n",
       "   'c49',\n",
       "   'c50',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c55',\n",
       "   'c59',\n",
       "   'c61',\n",
       "   'c62',\n",
       "   'c63',\n",
       "   'c65',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c69',\n",
       "   'c70',\n",
       "   'c71',\n",
       "   'c72',\n",
       "   'c74',\n",
       "   'c75',\n",
       "   'c77',\n",
       "   'c79',\n",
       "   'c7_lag_1',\n",
       "   'c10_lag_1',\n",
       "   'c13_lag_1',\n",
       "   'c15_lag_1',\n",
       "   'c16_lag_1',\n",
       "   'c17_lag_1',\n",
       "   'c25_lag_1',\n",
       "   'c26_lag_1',\n",
       "   'c27_lag_1',\n",
       "   'c28_lag_1',\n",
       "   'c35_lag_1',\n",
       "   'c40_lag_1',\n",
       "   'c44_lag_1',\n",
       "   'c45_lag_1',\n",
       "   'c48_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c52_lag_1',\n",
       "   'c54_lag_1',\n",
       "   'c55_lag_1',\n",
       "   'c56_lag_1',\n",
       "   'c57_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c60_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c65_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c69_lag_1',\n",
       "   'c71_lag_1',\n",
       "   'c3_lag_2',\n",
       "   'c4_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c25_lag_2',\n",
       "   'c39_lag_2',\n",
       "   'c44_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c50_lag_2',\n",
       "   'c51_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c58_lag_2',\n",
       "   'c62_lag_2',\n",
       "   'c63_lag_2',\n",
       "   'c64_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c70_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c77_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c10_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c21_lag_3',\n",
       "   'c22_lag_3',\n",
       "   'c23_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c31_lag_3',\n",
       "   'c33_lag_3',\n",
       "   'c38_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c42_lag_3',\n",
       "   'c53_lag_3',\n",
       "   'c55_lag_3',\n",
       "   'c56_lag_3',\n",
       "   'c57_lag_3',\n",
       "   'c60_lag_3',\n",
       "   'c61_lag_3',\n",
       "   'c62_lag_3',\n",
       "   'c65_lag_3',\n",
       "   'c66_lag_3',\n",
       "   'c70_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c80_lag_3',\n",
       "   'c3_lag_4',\n",
       "   'c5_lag_4',\n",
       "   'c10_lag_4',\n",
       "   'c12_lag_4',\n",
       "   'c13_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c18_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c31_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c37_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c50_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c54_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c67_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c75_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c5_lag_5',\n",
       "   'c7_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c9_lag_5',\n",
       "   'c16_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c21_lag_5',\n",
       "   'c24_lag_5',\n",
       "   'c25_lag_5',\n",
       "   'c30_lag_5',\n",
       "   'c33_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c39_lag_5',\n",
       "   'c43_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c46_lag_5',\n",
       "   'c47_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c62_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c66_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c71_lag_5'],\n",
       "  'n_features': 198},\n",
       " 75: {'alpha': 0.006783352936274421,\n",
       "  'selected_features': array([  6,  10,  11,  15,  16,  18,  24,  31,  37,  39,  45,  50,  51,\n",
       "          57,  58,  59,  66,  70,  75,  77,  84, 101, 118, 125, 146, 147,\n",
       "         165, 167, 168, 173, 174, 175, 192, 208, 248, 253, 255, 265, 276,\n",
       "         297, 315, 318, 326, 333, 336, 338, 347, 353, 354, 399, 400, 401,\n",
       "         402, 434, 449, 454, 459, 462, 476]),\n",
       "  'r2_score': 0.9901416776085569,\n",
       "  'selected_features_names': ['c7',\n",
       "   'c11',\n",
       "   'c12',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c19',\n",
       "   'c25',\n",
       "   'c32',\n",
       "   'c38',\n",
       "   'c40',\n",
       "   'c46',\n",
       "   'c51',\n",
       "   'c52',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c67',\n",
       "   'c71',\n",
       "   'c76',\n",
       "   'c78',\n",
       "   'c5_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c39_lag_1',\n",
       "   'c46_lag_1',\n",
       "   'c67_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c8_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c14_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c33_lag_2',\n",
       "   'c49_lag_2',\n",
       "   'c9_lag_3',\n",
       "   'c14_lag_3',\n",
       "   'c16_lag_3',\n",
       "   'c26_lag_3',\n",
       "   'c37_lag_3',\n",
       "   'c58_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c7_lag_4',\n",
       "   'c14_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c34_lag_4',\n",
       "   'c35_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c1_lag_5',\n",
       "   'c2_lag_5',\n",
       "   'c3_lag_5',\n",
       "   'c35_lag_5',\n",
       "   'c50_lag_5',\n",
       "   'c55_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 59},\n",
       " 76: {'alpha': 0.002968710190901865,\n",
       "  'selected_features': array([  8,  14,  15,  17,  18,  19,  23,  24,  35,  36,  38,  44,  47,\n",
       "          55,  56,  59,  61,  64,  69,  75,  76,  77, 101, 102, 103, 109,\n",
       "         128, 132, 138, 140, 141, 143, 159, 165, 169, 170, 171, 172, 176,\n",
       "         177, 178, 199, 204, 213, 215, 224, 228, 230, 233, 241, 251, 267,\n",
       "         271, 278, 306, 310, 313, 335, 390, 399, 401, 403, 407, 414, 417,\n",
       "         427, 430, 431, 450, 453, 457, 459, 464, 466, 467, 468, 469, 471,\n",
       "         474, 475, 476]),\n",
       "  'r2_score': 0.9964328782984598,\n",
       "  'selected_features_names': ['c9',\n",
       "   'c15',\n",
       "   'c16',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c24',\n",
       "   'c25',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c39',\n",
       "   'c45',\n",
       "   'c48',\n",
       "   'c56',\n",
       "   'c57',\n",
       "   'c60',\n",
       "   'c62',\n",
       "   'c65',\n",
       "   'c70',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c22_lag_1',\n",
       "   'c23_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c30_lag_1',\n",
       "   'c49_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c59_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c62_lag_1',\n",
       "   'c64_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c13_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c18_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c45_lag_2',\n",
       "   'c54_lag_2',\n",
       "   'c56_lag_2',\n",
       "   'c65_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c28_lag_3',\n",
       "   'c32_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c67_lag_3',\n",
       "   'c71_lag_3',\n",
       "   'c74_lag_3',\n",
       "   'c16_lag_4',\n",
       "   'c71_lag_4',\n",
       "   'c80_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c4_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c15_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c31_lag_5',\n",
       "   'c32_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c68_lag_5',\n",
       "   'c69_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c72_lag_5',\n",
       "   'c75_lag_5',\n",
       "   'c76_lag_5',\n",
       "   'c77_lag_5'],\n",
       "  'n_features': 81},\n",
       " 77: {'alpha': 0.002622680109147341,\n",
       "  'selected_features': array([ 15,  17,  18,  21,  26,  29,  31,  33,  35,  36,  37,  38,  39,\n",
       "          56,  57,  58,  59,  63,  65,  66,  70,  75,  76,  77,  78,  98,\n",
       "         129, 140, 147, 157, 159, 160, 169, 174, 175, 176, 193, 196, 205,\n",
       "         212, 227, 233, 241, 242, 243, 245, 250, 252, 259, 266, 278, 293,\n",
       "         314, 332, 334, 335, 336, 346, 347, 352, 360, 361, 363, 368, 374,\n",
       "         384, 387, 388, 393, 395, 402, 403, 407, 416, 417, 427, 433, 436,\n",
       "         439, 448, 453, 457, 460, 462, 463, 472, 474]),\n",
       "  'r2_score': 0.9956110824746996,\n",
       "  'selected_features_names': ['c16',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c22',\n",
       "   'c27',\n",
       "   'c30',\n",
       "   'c32',\n",
       "   'c34',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c57',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c64',\n",
       "   'c66',\n",
       "   'c67',\n",
       "   'c71',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c19_lag_1',\n",
       "   'c50_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c68_lag_1',\n",
       "   'c78_lag_1',\n",
       "   'c80_lag_1',\n",
       "   'c1_lag_2',\n",
       "   'c10_lag_2',\n",
       "   'c15_lag_2',\n",
       "   'c16_lag_2',\n",
       "   'c17_lag_2',\n",
       "   'c34_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c46_lag_2',\n",
       "   'c53_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c2_lag_3',\n",
       "   'c3_lag_3',\n",
       "   'c4_lag_3',\n",
       "   'c6_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c13_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c27_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c54_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c13_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c27_lag_4',\n",
       "   'c28_lag_4',\n",
       "   'c33_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c42_lag_4',\n",
       "   'c44_lag_4',\n",
       "   'c49_lag_4',\n",
       "   'c55_lag_4',\n",
       "   'c65_lag_4',\n",
       "   'c68_lag_4',\n",
       "   'c69_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c3_lag_5',\n",
       "   'c4_lag_5',\n",
       "   'c8_lag_5',\n",
       "   'c17_lag_5',\n",
       "   'c18_lag_5',\n",
       "   'c28_lag_5',\n",
       "   'c34_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c49_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c61_lag_5',\n",
       "   'c63_lag_5',\n",
       "   'c64_lag_5',\n",
       "   'c73_lag_5',\n",
       "   'c75_lag_5'],\n",
       "  'n_features': 87},\n",
       " 78: {'alpha': 0.0027435206259836488,\n",
       "  'selected_features': array([  3,   6,  14,  16,  17,  18,  19,  25,  34,  35,  36,  39,  43,\n",
       "          50,  53,  55,  56,  58,  67,  75,  76,  77,  78,  90,  93, 103,\n",
       "         115, 132, 140, 142, 149, 158, 164, 165, 166, 168, 170, 171, 180,\n",
       "         196, 199, 218, 226, 228, 230, 231, 233, 237, 240, 244, 250, 251,\n",
       "         259, 268, 269, 275, 280, 289, 308, 311, 312, 314, 318, 320, 334,\n",
       "         335, 338, 339, 345, 349, 364, 366, 370, 371, 377, 395, 408, 410,\n",
       "         411, 412, 419, 443, 444, 450, 452, 453, 456, 459, 464, 466, 469,\n",
       "         473]),\n",
       "  'r2_score': 0.9949043689665586,\n",
       "  'selected_features_names': ['c4',\n",
       "   'c7',\n",
       "   'c15',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c20',\n",
       "   'c26',\n",
       "   'c35',\n",
       "   'c36',\n",
       "   'c37',\n",
       "   'c40',\n",
       "   'c44',\n",
       "   'c51',\n",
       "   'c54',\n",
       "   'c56',\n",
       "   'c57',\n",
       "   'c59',\n",
       "   'c68',\n",
       "   'c76',\n",
       "   'c77',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c11_lag_1',\n",
       "   'c14_lag_1',\n",
       "   'c24_lag_1',\n",
       "   'c36_lag_1',\n",
       "   'c53_lag_1',\n",
       "   'c61_lag_1',\n",
       "   'c63_lag_1',\n",
       "   'c70_lag_1',\n",
       "   'c79_lag_1',\n",
       "   'c5_lag_2',\n",
       "   'c6_lag_2',\n",
       "   'c7_lag_2',\n",
       "   'c9_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c21_lag_2',\n",
       "   'c37_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c59_lag_2',\n",
       "   'c67_lag_2',\n",
       "   'c69_lag_2',\n",
       "   'c71_lag_2',\n",
       "   'c72_lag_2',\n",
       "   'c74_lag_2',\n",
       "   'c78_lag_2',\n",
       "   'c1_lag_3',\n",
       "   'c5_lag_3',\n",
       "   'c11_lag_3',\n",
       "   'c12_lag_3',\n",
       "   'c20_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c30_lag_3',\n",
       "   'c36_lag_3',\n",
       "   'c41_lag_3',\n",
       "   'c50_lag_3',\n",
       "   'c69_lag_3',\n",
       "   'c72_lag_3',\n",
       "   'c73_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c79_lag_3',\n",
       "   'c1_lag_4',\n",
       "   'c15_lag_4',\n",
       "   'c16_lag_4',\n",
       "   'c19_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c26_lag_4',\n",
       "   'c30_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c47_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c58_lag_4',\n",
       "   'c76_lag_4',\n",
       "   'c9_lag_5',\n",
       "   'c11_lag_5',\n",
       "   'c12_lag_5',\n",
       "   'c13_lag_5',\n",
       "   'c20_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c45_lag_5',\n",
       "   'c51_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c54_lag_5',\n",
       "   'c57_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c65_lag_5',\n",
       "   'c67_lag_5',\n",
       "   'c70_lag_5',\n",
       "   'c74_lag_5'],\n",
       "  'n_features': 92},\n",
       " 79: {'alpha': 0.0032506622887274314,\n",
       "  'selected_features': array([  6,   8,  15,  16,  17,  18,  30,  31,  32,  37,  38,  39,  48,\n",
       "          50,  52,  53,  55,  57,  58,  59,  75,  77,  78,  98,  99, 100,\n",
       "         101, 121, 130, 145, 165, 170, 171, 178, 199, 200, 201, 225, 227,\n",
       "         252, 268, 278, 314, 315, 328, 336, 339, 342, 343, 358, 360, 364,\n",
       "         365, 370, 371, 393, 401, 409, 413, 436, 437, 439, 440, 441, 443,\n",
       "         452, 457, 459, 464]),\n",
       "  'r2_score': 0.9976105233457395,\n",
       "  'selected_features_names': ['c7',\n",
       "   'c9',\n",
       "   'c16',\n",
       "   'c17',\n",
       "   'c18',\n",
       "   'c19',\n",
       "   'c31',\n",
       "   'c32',\n",
       "   'c33',\n",
       "   'c38',\n",
       "   'c39',\n",
       "   'c40',\n",
       "   'c49',\n",
       "   'c51',\n",
       "   'c53',\n",
       "   'c54',\n",
       "   'c56',\n",
       "   'c58',\n",
       "   'c59',\n",
       "   'c60',\n",
       "   'c76',\n",
       "   'c78',\n",
       "   'c79',\n",
       "   'c19_lag_1',\n",
       "   'c20_lag_1',\n",
       "   'c21_lag_1',\n",
       "   'c22_lag_1',\n",
       "   'c42_lag_1',\n",
       "   'c51_lag_1',\n",
       "   'c66_lag_1',\n",
       "   'c6_lag_2',\n",
       "   'c11_lag_2',\n",
       "   'c12_lag_2',\n",
       "   'c19_lag_2',\n",
       "   'c40_lag_2',\n",
       "   'c41_lag_2',\n",
       "   'c42_lag_2',\n",
       "   'c66_lag_2',\n",
       "   'c68_lag_2',\n",
       "   'c13_lag_3',\n",
       "   'c29_lag_3',\n",
       "   'c39_lag_3',\n",
       "   'c75_lag_3',\n",
       "   'c76_lag_3',\n",
       "   'c9_lag_4',\n",
       "   'c17_lag_4',\n",
       "   'c20_lag_4',\n",
       "   'c23_lag_4',\n",
       "   'c24_lag_4',\n",
       "   'c39_lag_4',\n",
       "   'c41_lag_4',\n",
       "   'c45_lag_4',\n",
       "   'c46_lag_4',\n",
       "   'c51_lag_4',\n",
       "   'c52_lag_4',\n",
       "   'c74_lag_4',\n",
       "   'c2_lag_5',\n",
       "   'c10_lag_5',\n",
       "   'c14_lag_5',\n",
       "   'c37_lag_5',\n",
       "   'c38_lag_5',\n",
       "   'c40_lag_5',\n",
       "   'c41_lag_5',\n",
       "   'c42_lag_5',\n",
       "   'c44_lag_5',\n",
       "   'c53_lag_5',\n",
       "   'c58_lag_5',\n",
       "   'c60_lag_5',\n",
       "   'c65_lag_5'],\n",
       "  'n_features': 69}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_components = ['c'+str(i) for i in range(1,total_comps+1)]\n",
    "\n",
    "selected_components = []\n",
    "for lag in range(0, 6):\n",
    "    for i in range(total_comps):\n",
    "        if lag == 0:\n",
    "            selected_components.append(f'c{i+1}')\n",
    "        else:  \n",
    "            selected_components.append(f'c{i+1}_lag_{lag}')\n",
    "\n",
    "for i in selected_components_lasso.keys():\n",
    "    selected_components_lasso[i]['selected_features_names'] = [selected_components[i] for i in selected_components_lasso[i]['selected_features']]\n",
    "    selected_components_lasso[i]['n_features'] = len(selected_components_lasso[i]['selected_features'])\n",
    "\n",
    "selected_components_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = './runs/pcmci_results/variable_selection_LASSO_%s_components-months-%s_%s.bin' % (total_comps, months, mask)\n",
    "file_name = './runs/pcmci_results/variable_selection_LASSO_%s_3dm_%s-comps-%s_months-%s_%s.bin' % (model, total_comps, str(list(n_comps.values())), months, mask)\n",
    "\n",
    "file = open(file_name, 'wb')\n",
    "pickle.dump(selected_components_lasso, file, protocol=-1)        \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
